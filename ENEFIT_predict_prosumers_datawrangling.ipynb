{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext cudf.pandas\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import holidays\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#For kaggle, GPT-4 suggests:\n",
    "from flask import Flask, jsonify\n",
    "import kaggle\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/datasets', methods=['GET'])\n",
    "def get_datasets():\n",
    "    # This is a basic example. You should add error handling and more specific functionality.\n",
    "    datasets = kaggle.api.datasets_list()  # Use Kaggle's API to get dataset info\n",
    "    return jsonify(datasets)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def reference_score(true_values, predicted_values):\n",
    "    score = mean_absolute_error(true_values, predicted_values)\n",
    "    return score\n",
    "\n",
    "\n",
    "def apply_dbscan_clustering(df, columns):\n",
    "    # Check if columns are in the DataFrame\n",
    "    if not all(col in df.columns for col in columns):\n",
    "        raise ValueError(\"One or more specified columns are not in the DataFrame\")\n",
    "\n",
    "    # Standardizing the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns])\n",
    "\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN()\n",
    "    clusters = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "    # Add cluster labels to the DataFrame\n",
    "    df['cluster_label'] = clusters\n",
    "\n",
    "    return df, dbscan, scaler\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# clustered_df, trained_dbscan, scaler = apply_dbscan_clustering(df, ['column1', 'column2'])\n",
    "\n",
    "def predict_dbscan_clusters(new_df, columns, trained_dbscan, scaler):\n",
    "    # Check if columns are in the DataFrame\n",
    "    if not all(col in new_df.columns for col in columns):\n",
    "        raise ValueError(\"One or more specified columns are not in the DataFrame\")\n",
    "\n",
    "    # Standardizing the new data using the previously fitted scaler\n",
    "    scaled_data = scaler.transform(new_df[columns])\n",
    "\n",
    "    # Predict clusters using the trained DBSCAN model\n",
    "    new_clusters = trained_dbscan.fit_predict(scaled_data)\n",
    "\n",
    "    # Add cluster labels to the new DataFrame\n",
    "    new_df['predicted_cluster_label'] = new_clusters\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Example usage:\n",
    "# new_df = pd.read_csv('new_data.csv')\n",
    "# predicted_df = predict_dbscan_clusters(new_df, ['column1', 'column2'], trained_dbscan, scaler)\n",
    "\n",
    "\n",
    "def train_lstm(df, input_columns, output_column, N, M):\n",
    "    # Create shifted columns for each input\n",
    "    for col in input_columns:\n",
    "        for n in range(1, N + 1):\n",
    "            df[f'{col}_shifted_{n}'] = df[col].shift(n)\n",
    "\n",
    "    # Create shifted columns for the output\n",
    "    for m in range(1, M + 1):\n",
    "        df[f'{output_column}_shifted_{m}'] = df[output_column].shift(m)\n",
    "\n",
    "    # Drop rows with NaN values (due to shifting)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Separate the original output column\n",
    "    original_output = df[output_column]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df.drop(columns=[output_column]))\n",
    "\n",
    "    # Prepare the dataset for the LSTM\n",
    "    X = df_scaled\n",
    "    y = original_output.values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Reshape input to be 3D [samples, timesteps, features] as required by LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(1, X_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f'Model Score (MSE): {score}')\n",
    "    return model, score, scaler\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')  # Load your DataFrame here\n",
    "# model, score, scaler = train_lstm(df, ['input1', 'input2'], 'output', N=3, M=1)\n",
    "\n",
    "\n",
    "def test_lstm(model, scaler, df, input_columns, output_column, N, M):\n",
    "    # Apply the same time shifts to input columns\n",
    "    for col in input_columns:\n",
    "        for n in range(1, N + 1):\n",
    "            df[f'{col}_shifted_{n}'] = df[col].shift(n)\n",
    "\n",
    "    # Apply the same time shifts to output column\n",
    "    for m in range(1, M + 1):\n",
    "        df[f'{output_column}_shifted_{m}'] = df[output_column].shift(m)\n",
    "\n",
    "    # Drop rows with NaN values (due to shifting)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Standardize the data using the scaler from training\n",
    "    df_scaled = scaler.transform(df.drop(columns=[output_column]))\n",
    "\n",
    "    # Reshape input to be 3D as required by LSTM\n",
    "    X = df_scaled.reshape((df_scaled.shape[0], 1, df_scaled.shape[1]))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "# new_df = pd.read_csv('new_data.csv')  # Load new data\n",
    "# predictions = test_lstm(trained_model, trained_scaler, new_df, ['input1', 'input2'], 'output', N=3, M=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and merging data\n",
    "Here the [Kaggle dataset description](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_frequency(df,dtcol):\n",
    "    # Convert the 'datetime' column to a DatetimeIndex if it's not already\n",
    "    df[dtcol] = pd.to_datetime(df[dtcol])\n",
    "\n",
    "    # Set the 'datetime' column as the index of the DataFrame\n",
    "    df = df.set_index(dtcol)\n",
    "\n",
    "    # Infer the frequency of the DatetimeIndex\n",
    "    frequency = pd.infer_freq(df.index[:100])  # Using a slice of the index to infer frequency\n",
    "\n",
    "    print(f\"The inferred frequency of the datetime column is: {frequency}\")\n",
    "\n",
    "\n",
    "def find_festives_estonia(df,datetime):\n",
    "    es_holidays=holidays.Estonia()\n",
    "    df['es_festive']=0\n",
    "    for holiday in es_holidays:\n",
    "        df['es_festive'].iloc[df[datetime]==holiday]=1\n",
    "    return(df)\n",
    "\n",
    "def lat_long_county(n,what='all',data_path=''):\n",
    "    ##import weather_station_to_county_mapping!!\n",
    "    #df=pd.read_csv(data_path+r'\\weather_station_to_county_mapping.csv')\n",
    "    #df=df.set_index('county')\n",
    "    \n",
    "    county_dict={0:\"HARJUMAA\",\\\n",
    "                 1:\"HIIUMAA\",\\\n",
    "                2:\"IDA-VIRUMAA\",\\\n",
    "                3:\"J\\u00c4RVAMAA\",\\\n",
    "                4:\"J\\u00d5GEVAMAA\",\\\n",
    "                5:\"L\\u00c4\\u00c4NE-VIRUMAA\",\\\n",
    "                6:\"L\\u00c4\\u00c4NEMAA\",\\\n",
    "                7:\"P\\u00c4RNUMAA\",\\\n",
    "                8:\"P\\u00d5LVAMAA\",\\\n",
    "                9:\"RAPLAMAA\",\\\n",
    "                10:\"SAAREMAA\",\\\n",
    "                11:\"TARTUMAA\",\\\n",
    "                12:\"UNKNOWN\",\\\n",
    "                13:\"VALGAMAA\",\\\n",
    "                14:\"VILJANDIMAA\",\\\n",
    "                15:\"V\\u00d5RUMAA\"}\n",
    "    df=pd.DataFrame(index=county_dict.keys(), columns=['lat','long'])\n",
    "\n",
    "    df.loc[0]=[59.416665,24.749997]\n",
    "    df.loc[1]=[58.923955,22.591947]\n",
    "    df.loc[2]=[59.2166658,27.2999988]\n",
    "    df.loc[3]=[58.8833298,25.5499978]\n",
    "    df.loc[4]=[58.74667,26.39389]\n",
    "    df.loc[5]=[59.32564740,26.63369150]\n",
    "    df.loc[6]=[58.916663,23.749997]\n",
    "    df.loc[7]=[58.38588,24.49711]\n",
    "    df.loc[8]=[58.06028,27.06944]\n",
    "    df.loc[9]=[59.00722,24.79278]\n",
    "    df.loc[10]=[58.416665,22.583331]\n",
    "    df.loc[11]=[58.378025, 26.728493]\n",
    "    df.loc[13]=[57.77781,26.0473]\n",
    "    df.loc[14]=[58.36389,25.59]\n",
    "    df.loc[15]=[57.83389, 27.01944]\n",
    "    df['lat'].loc[12]=df['lat'].mean()\n",
    "    df['long'].loc[12]=df['long'].mean()\n",
    "    '''\n",
    "    for item in county_dict:\n",
    "        df['county_name']=county_dict[item]\n",
    "    '''\n",
    "    if what=='all':\n",
    "        res=df.loc[n]\n",
    "    elif what=='lat':\n",
    "        res=df.loc[n]\n",
    "        res=res['lat']\n",
    "    elif what=='long':\n",
    "        res=df.loc[n]\n",
    "        res=res['long']\n",
    "    \n",
    "    return(res)\n",
    "\n",
    "\n",
    "def lat_long_all(data_path=''):\n",
    "    ##import weather_station_to_county_mapping!!\n",
    "    #df=pd.read_csv(data_path+r'\\weather_station_to_county_mapping.csv')\n",
    "    #df=df.set_index('county')\n",
    "    colat='latitude'\n",
    "    colong='longitude'\n",
    "    county_dict={0:\"HARJUMAA\",\\\n",
    "                 1:\"HIIUMAA\",\\\n",
    "                2:\"IDA-VIRUMAA\",\\\n",
    "                3:\"J\\u00c4RVAMAA\",\\\n",
    "                4:\"J\\u00d5GEVAMAA\",\\\n",
    "                5:\"L\\u00c4\\u00c4NE-VIRUMAA\",\\\n",
    "                6:\"L\\u00c4\\u00c4NEMAA\",\\\n",
    "                7:\"P\\u00c4RNUMAA\",\\\n",
    "                8:\"P\\u00d5LVAMAA\",\\\n",
    "                9:\"RAPLAMAA\",\\\n",
    "                10:\"SAAREMAA\",\\\n",
    "                11:\"TARTUMAA\",\\\n",
    "                12:\"UNKNOWN\",\\\n",
    "                13:\"VALGAMAA\",\\\n",
    "                14:\"VILJANDIMAA\",\\\n",
    "                15:\"V\\u00d5RUMAA\"}\n",
    "    df=pd.DataFrame(index=county_dict.keys(), columns=[colat,colong])\n",
    "\n",
    "    df.loc[0]=[59.416665,24.749997]\n",
    "    df.loc[1]=[58.923955,22.591947]\n",
    "    df.loc[2]=[59.2166658,27.2999988]\n",
    "    df.loc[3]=[58.8833298,25.5499978]\n",
    "    df.loc[4]=[58.74667,26.39389]\n",
    "    df.loc[5]=[59.32564740,26.63369150]\n",
    "    df.loc[6]=[58.916663,23.749997]\n",
    "    df.loc[7]=[58.38588,24.49711]\n",
    "    df.loc[8]=[58.06028,27.06944]\n",
    "    df.loc[9]=[59.00722,24.79278]\n",
    "    df.loc[10]=[58.416665,22.583331]\n",
    "    df.loc[11]=[58.378025, 26.728493]\n",
    "    df.loc[13]=[57.77781,26.0473]\n",
    "    df.loc[14]=[58.36389,25.59]\n",
    "    df.loc[15]=[57.83389, 27.01944]\n",
    "    df[colat].loc[12]=df[colat].mean()\n",
    "    df[colong].loc[12]=df[colong].mean()\n",
    "    '''\n",
    "    for item in county_dict:\n",
    "        df['county_name']=county_dict[item]\n",
    "    '''\n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def lat_county(n):\n",
    "    lat=lat_long_county(n,what='lat')\n",
    "    return(lat)\n",
    "\n",
    "def long_county(n):\n",
    "    long=lat_long_county(n,what='long')\n",
    "    return(long)\n",
    "\n",
    "    \n",
    "    \n",
    "def findClosest(arr, n, target):\n",
    "    left, right = 0, n - 1\n",
    "    while left < right:\n",
    "        if abs(arr[left] - target) <= abs(arr[right] - target):\n",
    "            right -= 1\n",
    "        else:\n",
    "            left += 1\n",
    "    return (arr[left],arr[left+1])\n",
    "def weighted_value(x,xm,xM,ym,yM):\n",
    "    if x==xM or x==xm:\n",
    "        y=(ym+yM)/2\n",
    "    else:\n",
    "        y=(x-xm)/(xM-xm)*(yM-ym)+ym\n",
    "    y=np.round(y, 2)\n",
    "    return(y)\n",
    "\n",
    "def find_weather_on_lat_long_old(weather_df,lat,long,parameters_to_extract,parameters_to_keep,datetime='datetime',datetime2='datetime2',col_lat='latitude',col_long='longitude'):\n",
    "    #find closest lat and long in forecast weather\n",
    "    [latm,latM]=findClosest(weather_df.groupby(col_lat).mean().index,len(weather_df.groupby(col_lat).mean().index),lat)\n",
    "    [lonm,lonM]=findClosest(weather_df.groupby(col_long).mean().index,len(weather_df.groupby(col_long).mean().index),long)\n",
    "    #tmp_df is a dataframe containing all predictions for the little swquare latlong\n",
    "    #Useful to avoid data out of bound. \n",
    "    temp_df=weather_df[weather_df[col_lat]>=latm]\n",
    "    temp_df=temp_df[temp_df[col_lat]<=latM]\n",
    "    temp_df=temp_df[temp_df[col_long]<=lonM]\n",
    "    temp_df=temp_df[temp_df[col_long]>=lonm]\n",
    "    #predictions are from different days. I need to account for datetime\n",
    "    dates=temp_df.groupby(datetime).mean().index\n",
    "    df=pd.DataFrame(index=dates,columns=parameters_to_extract+parameters_to_keep)\n",
    "\n",
    "\n",
    "\n",
    "    for col in parameters_to_extract:\n",
    "        df[col]=np.nan\n",
    "        print(col)\n",
    "        for data in dates:\n",
    "            squared_data=temp_df[temp_df[datetime]==data]\n",
    "\n",
    "            if len(squared_data)>4:\n",
    "                #48 hours predictions mean that at midnight day 0 you have preds up to midnight3. Keep the latest. \n",
    "                indexlist=pd.to_datetime(squared_data[datetime2].values).day==np.max(pd.to_datetime(squared_data[datetime2].values).day)\n",
    "\n",
    "                squared_data=squared_data.loc[indexlist]\n",
    "\n",
    "            #find weighted parameter based on lat and long. Could be checked as below. \n",
    "\n",
    "            squared=squared_data[squared_data[col_long]==lonM]\n",
    "            T1=weighted_value(lat,latm,latM,\n",
    "                              squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "\n",
    "            squared=squared_data[squared_data[col_long]==lonm]\n",
    "\n",
    "            T2=weighted_value(lat,latm,latM,squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "            #place in the right spot of temp the right value.\n",
    "            df[col].loc[data]=weighted_value(long,lonm,lonM,float(T2),float(T1))\n",
    "            for colkeep in parameters_to_keep:\n",
    "                if colkeep==datetime:\n",
    "                    df[colkeep]=data\n",
    "                else:\n",
    "                    df[colkeep]=squared_data[colkeep].iloc[0]\n",
    "            df[col_lat]=lat\n",
    "            df[col_long]=long\n",
    "    #The final dataset contains one row per datetime and one column per parameter. \n",
    "    #Can me merged duplicating rows in the train dataset.\n",
    "    return(df)\n",
    "    '''\n",
    "    temp3=temp2 in lat ==latM\n",
    "    T3=weighted(lon,lonm,lonM,T@onm,T@lonM)\n",
    "    temp3=temp2 in lon ==latm\n",
    "    T4=weighted(lon,lonm,lonM,T@onm,T@lonM)\n",
    "    Tref2=weighted(lat,latm,latM,T4,T3)\n",
    "\n",
    "    if Tref1=! Tref2:\n",
    "    Tref=mean(Tref1,Tref2)\n",
    "    else print('ok')\n",
    "\n",
    "    temp[col].iloc[data]=Tref\n",
    "    '''\n",
    "    \n",
    "def find_weather_on_lat_long(weather_df, lat, long, parameters_to_extract, parameters_to_keep,\n",
    "                             datetime='datetime', datetime2='datetime2', col_lat='latitude', col_long='longitude'):\n",
    "    # Precomputed groups (call these outside and pass them to the function if they don't change)\n",
    "    lat_group = weather_df.groupby(col_lat).mean().index\n",
    "    long_group = weather_df.groupby(col_long).mean().index\n",
    "\n",
    "    [latm, latM] = findClosest(lat_group, len(lat_group), lat)\n",
    "    [lonm, lonM] = findClosest(long_group, len(long_group), long)\n",
    "\n",
    "    # Filter the DataFrame in a single step\n",
    "    temp_df = weather_df[(weather_df[col_lat].between(latm, latM)) & (weather_df[col_long].between(lonm, lonM))]\n",
    "\n",
    "    # Create DataFrame\n",
    "    dates = temp_df.groupby(datetime).mean().index\n",
    "    df = pd.DataFrame(index=dates, columns=parameters_to_extract + parameters_to_keep)\n",
    "    \n",
    "    for col in parameters_to_extract:\n",
    "        print('Extracting {}'.format(col))\n",
    "        df[col] = np.nan\n",
    "        for data in dates:\n",
    "            squared_data = temp_df[temp_df[datetime] == data]\n",
    "\n",
    "            if len(squared_data) > 4:\n",
    "                max_day = pd.to_datetime(squared_data[datetime2].values).day.max()\n",
    "                squared_data = squared_data[pd.to_datetime(squared_data[datetime2].values).day == max_day]\n",
    "\n",
    "            for col_long_val in [lonm, lonM]:\n",
    "                squared = squared_data[squared_data[col_long] == col_long_val]\n",
    "                T = weighted_value(lat, latm, latM,\n",
    "                                   squared[squared[col_lat] == latm][col].values,\n",
    "                                   squared[squared[col_lat] == latM][col].values)\n",
    "                df.at[data, col] = T if col_long_val == lonm else weighted_value(long, lonm, lonM, float(T), float(df.at[data, col]))\n",
    "            \n",
    "            for colkeep in parameters_to_keep:\n",
    "                if colkeep == datetime:\n",
    "                    df[colkeep] = data\n",
    "                else:\n",
    "                    df[colkeep] = squared_data[colkeep].iloc[0]\n",
    "\n",
    "            df[col_lat] = lat\n",
    "            df[col_long] = long\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_weather_and_train(train,weather_df,lat_lon_data_train,lat_lon_data_w):\n",
    "    #lists of columns should contain in the right order:\n",
    "    # latitude,longitude,data.\n",
    "    #the function merges rows which have the same lat and long and same data \n",
    "    # - they are alla vailable in same time and same space. The dat ablock id is not taken into consderation yet.\n",
    "    train['mergecolumn']=np.nan\n",
    "    weather_df['mergecolumn']=np.nan\n",
    "    for index in train.index:\n",
    "        tempdt=pd.to_datetime(train[lat_lon_data_train[2]].loc[index])\n",
    "        val='{}{}{}{}'.format(tempdt.year,tempdt.month,tempdt.day,tempdt.hour)\n",
    "        train['mergecolumn'].loc[index]='{}_{}_{}'.\\\n",
    "            format(train[lat_lon_data_train[0]].loc[index],\n",
    "                   train[lat_lon_data_train[1]].loc[index],\n",
    "                   val)\n",
    "    try:\n",
    "        weather_df=weather_df.drop(columns=[lat_lon_data_w[2]])\n",
    "    except:\n",
    "        print('No duplicates')\n",
    "    weather_df=weather_df.reset_index(allow_duplicates=True)\n",
    "    for index in weather_df.index:\n",
    "        tempdt=pd.to_datetime(weather_df[lat_lon_data_w[2]].loc[index])\n",
    "        val='{}{}{}{}'.format(tempdt.year,tempdt.month,tempdt.day,tempdt.hour)\n",
    "        weather_df['mergecolumn'].loc[index]='{}_{}_{}'.\\\n",
    "            format(weather_df[lat_lon_data_w[0]].loc[index],\n",
    "                   weather_df[lat_lon_data_w[1]].loc[index],\n",
    "                   val)\n",
    "\n",
    "    merged_df = pd.merge(train, weather_df, on=['mergecolumn'], how='inner')\n",
    "\n",
    "    #merged_df=merged_df.drop_duplicates(subset=[data], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    #merged_df=merged_df.drop(['mergecolumn'], axis=1)\n",
    "\n",
    "    return(merged_df)\n",
    "\n",
    "\n",
    "def generate_dataframe(init_df,col,item):\n",
    "    df=init_df[init_df[col]==item]\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.csv\n",
    "\n",
    "- *county* - An ID code for the county.\n",
    "- *is_business* - Boolean for whether or not the prosumer is a business.\n",
    "- *product_type* - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n",
    "- *target* - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n",
    "- *is_consumption* - Boolean for whether or not this row's target is consumption or production.\n",
    "- *datetime* - The Estonian time in EET (UTC+2) / EEST (UTC+3).\n",
    "- *data_block_id* - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n",
    "- *row_id* - A unique identifier for the row.\n",
    "- *prediction_unit_id* - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n",
    "\n",
    "*prediction_unit_id* should not be used as input as it changes. \n",
    "It is interesting to understand if the physical behaviour is influenced by county, business and product type: uding a clustering and exluding those infos, then checking if clusters overlap with the items will give us some additiona infos.\n",
    "\n",
    "### client.csv\n",
    "\n",
    "- *product_type*\n",
    "- *county* - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n",
    "- *eic_count* - The aggregated number of consumption points (EICs - European Identifier Code).\n",
    "- *installed_capacity* - Installed photovoltaic solar panel capacity in kilowatts.\n",
    "- *is_business* - Boolean for whether or not the prosumer is a business.\n",
    "- *date*\n",
    "- *data_block_id*\n",
    "\n",
    "Installed capacity can be used to normalize production data. Production type, county and is business as well as data block id shoudl match the previous. \n",
    "To match client to train, we need to remap *prediction_unit_id* by finding county+productiontype+biz in the train dataset.\n",
    "\n",
    "### electricity_prices.csv\n",
    "\n",
    "- *origin_date*\n",
    "- *forecast_date*\n",
    "- *euros_per_mwh* - The price of electricity on the day ahead markets in euros per megawatt hour.\n",
    "- *data_block_id*\n",
    "\n",
    "How should prices will help me?\n",
    "\n",
    "### forecast_weather.csv \n",
    "Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n",
    "\n",
    "- *[latitude/longitude]* - The coordinates of the weather forecast.\n",
    "- *origin_datetime* - The timestamp of when the forecast was generated.\n",
    "- *hours_ahead* - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n",
    "- *temperature* - The air temperature at 2 meters above ground in degrees Celsius.\n",
    "- *dewpoint* - The dew point temperature at 2 meters above ground in degrees Celsius.\n",
    "- *cloudcover_[low/mid/high/total]* - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total.\n",
    "- *10_metre_[u/v]_wind_component* - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second.\n",
    "- *data_block_id*\n",
    "- *forecast_datetime* - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead.\n",
    "- *direct_solar_radiation* - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the preceding hour, in watt-hours per square meter.\n",
    "- *surface_solar_radiation_downwards* - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, in watt-hours per square meter.\n",
    "- *snowfall* - Snowfall over the previous hour in units of meters of water equivalent.\n",
    "- *total_precipitation* - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the preceding hour, in units of meters.\n",
    "\n",
    "### historical_weather.csv \n",
    "Historic weather data.\n",
    "\n",
    "- *datetime*\n",
    "- *temperature*\n",
    "- *dewpoint*\n",
    "- *rain* - Different from the forecast conventions. The rain from large scale weather systems of the preceding hour in millimeters.\n",
    "- *snowfall* - Different from the forecast conventions. Snowfall over the preceding hour in centimeters.\n",
    "- *surface_pressure* - The air pressure at surface in hectopascals.\n",
    "- *cloudcover_[low/mid/high/total]* - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n",
    "- *windspeed_10m* - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n",
    "- *winddirection_10m* - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n",
    "- *shortwave_radiation* - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n",
    "- *direct_solar_radiation*\n",
    "- *diffuse_radiation* - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n",
    "- *[latitude/longitude]* - The coordinates of the weather station.\n",
    "- *data_block_id*\n",
    "\n",
    "### Other data\n",
    "*public_timeseries_testing_util*.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it.\n",
    "\n",
    "*example_test_files/* Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three data_block_ids are repeats of the last three data_block_ids in the train set.\n",
    "\n",
    "*example_test_files/sample_submission.csv* A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission.\n",
    "\n",
    "*example_test_files/revealed_targets.csv* The actual target values from the day before the forecast time. This amounts to two days of lag relative to the prediction times in the test.csv.\n",
    "\n",
    "*enefit/* Files that enable the API. Expect the API to deliver all rows in under 15 minutes and to reserve less than 0.5 GB of memory. The copy of the API that you can download serves the data from example_test_files/. You must make predictions for those dates in order to advance the API but those predictions are not scored. Expect to see roughly three months of data delivered initially and up to ten months of data by the end of the forecasting period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###MAIN###\n",
    "\n",
    "data_path=r'C:\\Users\\Mipu_10\\Documents\\GitHub\\kaggle_enefit_prosumer_forecasting\\predict-energy-behavior-of-prosumers'\n",
    "train=pd.read_csv(data_path+r'\\train.csv')\n",
    "client=pd.read_csv(data_path+r'\\client.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order the train dataset based on prediction_unit_id\n",
    "train=train.sort_values('prediction_unit_id')\n",
    "\n",
    "#Split consumption and generation\n",
    "consumption=generate_dataframe(train,'is_consumption',1)\n",
    "production=generate_dataframe(train,'is_consumption',0)\n",
    "\n",
    "consumption_b=generate_dataframe(consumption,'is_business',1)\n",
    "consumption_c=generate_dataframe(consumption,'is_business',0)\n",
    "production_b=generate_dataframe(production,'is_business',1)\n",
    "production_c=generate_dataframe(production,'is_business',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#items of dictionary, will contain one dataframe per single production unit. Easier to manage. \n",
    "#dividing in 4 df is faster coz we donot work on very big dataframes. \n",
    "\n",
    "dfs=[consumption_b,consumption_c,production_b,production_c]\n",
    "what=['cb','cc','pb','pc']\n",
    "dict={}\n",
    "ii=0\n",
    "for df in dfs:\n",
    "    puis=df.groupby('prediction_unit_id').mean().index\n",
    "    \n",
    "    final_df=pd.DataFrame()\n",
    "    iters=0\n",
    "    for pui in puis:\n",
    "        temp=df[df['prediction_unit_id']==pui]\n",
    "        temp=temp.sort_values('datetime')\n",
    "        temp['datetime']=pd.to_datetime(temp['datetime'])\n",
    "        temp['weekday']=pd.to_datetime(temp['datetime'].values).weekday\n",
    "        temp['hour_of_day']=pd.to_datetime(temp['datetime'].values).hour\n",
    "        temp['day_of_month']=pd.to_datetime(temp['datetime'].values).day\n",
    "        temp['month']=pd.to_datetime(temp['datetime'].values).month\n",
    "        temp['date']=pd.to_datetime(temp['datetime'].values).date\n",
    "        temp=find_festives_estonia(temp,'datetime')\n",
    "        '''\n",
    "        if iters==0:\n",
    "            last=temp\n",
    "            iters=1\n",
    "        else:\n",
    "            last=pd.concat([last,temp])\n",
    "        '''\n",
    "        dict['{}'.format(pui)]=temp\n",
    "        ii=ii+1\n",
    "    \n",
    "#To avoid memory error, should truncate based on prediction unit or datetime. \n",
    "#Probably, the best thing to do is managing one prediction unit per time, and then batch training. \n",
    "for pui in dict: \n",
    "    print('--- Doing prediction unit {}'.format(pui))\n",
    "    df=dict[pui]\n",
    "    ttt='{}_{}_{}'.format('product_type','county','is_business')\n",
    "    client[ttt]=client['product_type']*100+client['county']*10+client['is_business']\n",
    "    df[ttt]=df['product_type']*100+df['county']*10+df['is_business']\n",
    "    iters=0\n",
    "    \n",
    "    #if dict contains one pui, this is useless. \n",
    "    #puis=df.groupby(ttt).mean().index\n",
    "    #N=3 #how many puis? max=len(puis)\n",
    "    \n",
    "    cols_to_import=['eic_count','installed_capacity','data_block_id',ttt]\n",
    "    #max_datetime=pd.to_datetime('2021-11-01 00:00:00')\n",
    "    #truncated_df=df[df['datetime']<max_datetime]\n",
    "    #for pui in puis[0:N]:\n",
    "\n",
    "    #dfpui=truncated_df[truncated_df[ttt]==pui]\n",
    "    clientpui=client[client[ttt]==pui][cols_to_import]\n",
    "    #tempmerged=pd.merge(dfpui,clientpui,on=ttt,how='left')\n",
    "    tempmerged=pd.merge(df,clientpui,on=ttt,how='left')\n",
    "    tempmerged=tempmerged.drop_duplicates(subset=['datetime'])\n",
    "    '''\n",
    "    if iters==0:\n",
    "        merged=tempmerged\n",
    "        iters=1\n",
    "    else:\n",
    "        merged=pd.concat([merged,tempmerged])\n",
    "    '''\n",
    "    dict[pui]=tempmerged\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather=pd.read_csv(data_path+r'\\forecast_weather.csv')\n",
    "historical_weather=pd.read_csv(data_path+r'\\historical_weather.csv')\n",
    "electricity_prices=pd.read_csv(data_path+r'\\electricity_prices.csv')\n",
    "\n",
    "for col in forecast_weather.columns:\n",
    "    forecast_weather=forecast_weather.rename(columns={col: \"{}_f\".format(col)})\n",
    "    \n",
    "for col in historical_weather.columns:\n",
    "    historical_weather=historical_weather.rename(columns={col: \"{}_h\".format(col)})\n",
    "    \n",
    "electricity_prices['forecast_date']=pd.to_datetime(electricity_prices['forecast_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the final dataframe. \n",
    "all_h_w_cols=['datetime_h', 'temperature_h', 'dewpoint_h', 'rain_h', 'snowfall_h',\n",
    "       'surface_pressure_h', 'cloudcover_total_h', 'cloudcover_low_h',\n",
    "       'cloudcover_mid_h', 'cloudcover_high_h', 'windspeed_10m_h',\n",
    "       'winddirection_10m_h', 'shortwave_radiation_h',\n",
    "       'direct_solar_radiation_h', 'diffuse_radiation_h', 'latitude_h',\n",
    "       'longitude_h', 'data_block_id_h']\n",
    "\n",
    "all_f_w_cols=['latitude_f', 'longitude_f', 'origin_datetime_f', 'hours_ahead_f',\n",
    "       'temperature_f', 'dewpoint_f', 'cloudcover_high_f', 'cloudcover_low_f',\n",
    "       'cloudcover_mid_f', 'cloudcover_total_f', '10_metre_u_wind_component_f',\n",
    "       '10_metre_v_wind_component_f', 'data_block_id_f', 'forecast_datetime_f',\n",
    "       'direct_solar_radiation_f', 'surface_solar_radiation_downwards_f',\n",
    "       'snowfall_f', 'total_precipitation_f']\n",
    "\n",
    "\n",
    "final_dict={}\n",
    "colslat=['latitude','longitude']\n",
    "test_f=0\n",
    "test_h=0\n",
    "#item='cb'\n",
    "\n",
    "counties=train.groupby('county').mean().index\n",
    "lldf=lat_long_all() #contains one index per county, latitude and longitude columns. \n",
    "\n",
    "#max_datetime=pd.to_datetime('2021-11-01 00:00:00')\n",
    "#df=tot_df[tot_df['datetime']<max_datetime]\n",
    "'''\n",
    "for row in df.index:\n",
    "    try:\n",
    "        df[colslat[0]].iloc[row],df[colslat[1]].iloc[row]=lat_long_county(df['county'].iloc[row])\n",
    "    except:\n",
    "        df[colslat[0]].iloc[row],df[colslat[1]].iloc[row]=lat_long_county(df['county'].iloc[row].values)\n",
    "'''\n",
    "print('I created lats and longs. Exploring weather.')\n",
    "\n",
    "#puis=df.groupby('prediction_unit_id').mean().index\n",
    "\n",
    "#df=df[df['prediction_unit_id']==puis[0]]\n",
    "\n",
    "lats=lldf.groupby('latitude').mean().index\n",
    "\n",
    "for lat in lats:\n",
    "    lons=lldf[lldf['latitude']==lat].groupby('longitude').mean().index\n",
    "    for long in lons:\n",
    "        print('{} and {}'.format(lat,long))\n",
    "        #find the dataset with all items at right lat and lon. \n",
    "\n",
    "        #forecasted\n",
    "        fweather_df=find_weather_on_lat_long(forecast_weather,lat,long,parameters_to_extract_f,parameters_to_keep_f,\n",
    "                                             datetime='forecast_datetime_f',datetime2='origin_datetime_f',col_lat='latitude_f',\n",
    "                                             col_long='longitude_f')\n",
    "        if test_f==0:\n",
    "            right_weather_forecast=fweather_df\n",
    "            test_f=1\n",
    "        else:\n",
    "            right_weather_forecast=pd.concat([right_weather_forecast,fweather_df])\n",
    "\n",
    "\n",
    "        #historical \n",
    "        hweather_df=find_weather_on_lat_long(historical_weather,lat,long,parameters_to_extract_h,parameters_to_keep_h,\n",
    "                                             datetime='datetime_h',datetime2='datetime_h',col_lat='latitude_h',\n",
    "                                             col_long='longitude_h')\n",
    "        if test_h==0:\n",
    "            right_weather_history=hweather_df\n",
    "            test_h=1\n",
    "        else:\n",
    "            right_weather_history=pd.concat([right_weather_history,hweather_df])\n",
    "            display(right_weather_history)\n",
    "\n",
    "\n",
    "        print('Found parameters for {} and {}'.format(lat,long))\n",
    "        #merge the right dweather df with train df\n",
    "\n",
    "        \n",
    "###--->> https://www.digitalocean.com/community/tutorials/pandas-dataframe-apply-examples impara qui a usarlo\n",
    "        \n",
    "        \n",
    "right_weather_forecast=right_weather_forecast.rename(columns={\"datetime_h\": \"datetime_h_original\"})\n",
    "right_weather_history=right_weather_history.rename(columns={\"forecast_datetime_f\": \"forecast_datetime_f_original\"})\n",
    "\n",
    "right_weather_forecast.to_pickle(\"./right_weather_forecast.pkl\")\n",
    "right_weather_history.to_pickle(\"./right_weather_history.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#forecast\n",
    "parameters_to_extract_f=['temperature_f', 'dewpoint_f', 'cloudcover_high_f', 'cloudcover_low_f',\n",
    "   'cloudcover_mid_f', 'cloudcover_total_f', '10_metre_u_wind_component_f',\n",
    "   '10_metre_v_wind_component_f','direct_solar_radiation_f', 'surface_solar_radiation_downwards_f',\n",
    "   'snowfall_f', 'total_precipitation_f']\n",
    "\n",
    "parameters_to_keep_f=['latitude_f', 'longitude_f', 'origin_datetime_f', \n",
    "                    'hours_ahead_f','data_block_id_f', 'forecast_datetime_f']#these need to be kept not weighted!\n",
    "\n",
    "#historical\n",
    "parameters_to_extract_h=['temperature_h', 'dewpoint_h', 'rain_h', 'snowfall_h',\n",
    "   'surface_pressure_h', 'cloudcover_total_h', 'cloudcover_low_h',\n",
    "   'cloudcover_mid_h', 'cloudcover_high_h', 'windspeed_10m_h',\n",
    "   'winddirection_10m_h', 'shortwave_radiation_h',\n",
    "   'direct_solar_radiation_h', 'diffuse_radiation_h']\n",
    "\n",
    "parameters_to_keep_h=['latitude_h', 'longitude_h', 'datetime_h', \n",
    "                    'data_block_id_h']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_data_train=['latitude','longitude','datetime']\n",
    "lat_lon_data_f=['latitude_f','longitude_f','forecast_datetime_f']\n",
    "lat_lon_data_h=['latitude_h','longitude_h','datetime_h']\n",
    "lat_lon_data_all=lat_lon_data_train+lat_lon_data_f\n",
    "for pui in dict:\n",
    "#pui=list(dict.keys())[0]\n",
    "    print('Doing {}'.format(pui))\n",
    "    df=dict[pui]\n",
    "    print('Defining lat and long')\n",
    "\n",
    "    df['latitude']=df['county']\n",
    "    df['latitude']= df['county'].apply(lat_county)\n",
    "\n",
    "\n",
    "    df['longitude']=df['county']\n",
    "    df['longitude'] = df['county'].apply(long_county)\n",
    "    \n",
    "    right_weather_forecast[lat_lon_data_f[2]]=pd.to_datetime(right_weather_forecast.index)\n",
    "    right_weather_history[lat_lon_data_h[2]]=pd.to_datetime(right_weather_history.index)\n",
    "\n",
    "    tempp=merge_weather_and_train(df,right_weather_forecast,lat_lon_data_train,lat_lon_data_f)\n",
    "    final_dict[pui]=merge_weather_and_train(tempp,right_weather_history,lat_lon_data_all,lat_lon_data_h)\n",
    "    final_dict[pui]=pd.merge(final_dict[pui], electricity_prices,  \n",
    "                               how='left', left_on=['datetime'], right_on = ['forecast_date'])\n",
    "    display(final_dict[pui])\n",
    "    print('finished merging {}.'.format(pui))\n",
    "    joblib.dump(final_dict, \"./final_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded!\n",
      "finished!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-fdcc76dbd4c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mfinal_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpui\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finished!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./train_dataset_complete.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m '''\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[0mNumpyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[0mNumpyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_framing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    993\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m                     \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 995\u001b[1;33m                     \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstate_setter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    993\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m                     \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 995\u001b[1;33m                     \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m             \u001b[1;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m             \u001b[1;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[1;31m# And then array bytes are written right after the wrapper.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(self, array, pickler)\u001b[0m\n\u001b[0;32m    132\u001b[0m                                            \u001b[0mbuffersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffersize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                                            order=self.order):\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "\n",
    "#if needed, load final dict.\n",
    "final_dict=joblib.load(r\"./final_dict.pkl\")\n",
    "print('loaded!')\n",
    "target_shifts=24\n",
    "norm='target_normalized'\n",
    "cols_to_shift_hourly=[norm,'temperature_f',\n",
    "'cloudcover_high_f',\n",
    "'cloudcover_low_f',\n",
    "'cloudcover_mid_f',\n",
    "'cloudcover_total_f',\n",
    "'direct_solar_radiation_f',\n",
    "'snowfall_f',\n",
    "'total_precipitation_f']\n",
    "cols_to_shift_daily=['es_festive']\n",
    "\n",
    "for pui in final_dict:\n",
    "    \n",
    "    df=final_dict[pui]\n",
    "    df=df.sort_values('datetime')\n",
    "    df[norm]=df['target']/df['installed_capacity']\n",
    "    for shift in range(1,target_shifts+1):\n",
    "        for col in cols_to_shift_hourly:\n",
    "            df['{}_{}'.format(col,shift)]=df[col].shift(shift)\n",
    "            df['{}_{}'.format(col,shift)]=df[col].shift(shift)\n",
    "    for col in cols_to_shift_daily:\n",
    "        df['{}_{}'.format(col,shift)]=df[col].shift(target_shifts+1)\n",
    "    final_dict[pui]=df\n",
    "print('finished!')\n",
    "joblib.dump(final_dict, \"./train_dataset_complete.pkl\")\n",
    "print('saved!')\n",
    "'''\n",
    "Next:\n",
    " - target shifts\n",
    " - correlation matrix\n",
    " - scatterplots\n",
    " - training!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(\"'{}',\".format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=['target_normalized']\n",
    "input=['county',\n",
    "'is_business',\n",
    "'product_type',\n",
    "'is_consumption',\n",
    "'weekday',\n",
    "'hour_of_day',\n",
    "'day_of_month',\n",
    "'month',\n",
    "'date',\n",
    "'es_festive',\n",
    "'product_type_county_is_business',\n",
    "'eic_count',\n",
    "'installed_capacity',\n",
    "'data_block_id_y',\n",
    "'latitude',\n",
    "'longitude',\n",
    "'forecast_datetime_f',\n",
    "'temperature_f',\n",
    "'dewpoint_f',\n",
    "'cloudcover_high_f',\n",
    "'cloudcover_low_f',\n",
    "'cloudcover_mid_f',\n",
    "'cloudcover_total_f',\n",
    "'10_metre_u_wind_component_f',\n",
    "'10_metre_v_wind_component_f',\n",
    "'direct_solar_radiation_f',\n",
    "'surface_solar_radiation_downwards_f',\n",
    "'snowfall_f',\n",
    "'total_precipitation_f',\n",
    "'origin_datetime_f',\n",
    "'hours_ahead_f',\n",
    "'data_block_id_f',\n",
    "'forecast_datetime_f_original',\n",
    "'euros_per_mwh',\n",
    "'target_normalized_1',\n",
    "'temperature_h_1',\n",
    "'rain_h_1',\n",
    "'target_normalized_2',\n",
    "'temperature_h_2',\n",
    "'rain_h_2',\n",
    "'target_normalized_3',\n",
    "'temperature_h_3',\n",
    "'rain_h_3',\n",
    "'target_normalized_4',\n",
    "'temperature_h_4',\n",
    "'rain_h_4',\n",
    "'target_normalized_5',\n",
    "'temperature_h_5',\n",
    "'rain_h_5',\n",
    "'target_normalized_6',\n",
    "'temperature_h_6',\n",
    "'rain_h_6',\n",
    "'target_normalized_7',\n",
    "'temperature_h_7',\n",
    "'rain_h_7',\n",
    "'target_normalized_8',\n",
    "'temperature_h_8',\n",
    "'rain_h_8',\n",
    "'target_normalized_9',\n",
    "'temperature_h_9',\n",
    "'rain_h_9',\n",
    "'target_normalized_10',\n",
    "'temperature_h_10',\n",
    "'rain_h_10',\n",
    "'target_normalized_11',\n",
    "'temperature_h_11',\n",
    "'rain_h_11',\n",
    "'target_normalized_12',\n",
    "'temperature_h_12',\n",
    "'rain_h_12',\n",
    "'target_normalized_13',\n",
    "'temperature_h_13',\n",
    "'rain_h_13',\n",
    "'target_normalized_14',\n",
    "'temperature_h_14',\n",
    "'rain_h_14',\n",
    "'target_normalized_15',\n",
    "'temperature_h_15',\n",
    "'rain_h_15',\n",
    "'target_normalized_16',\n",
    "'temperature_h_16',\n",
    "'rain_h_16',\n",
    "'target_normalized_17',\n",
    "'temperature_h_17',\n",
    "'rain_h_17',\n",
    "'target_normalized_18',\n",
    "'temperature_h_18',\n",
    "'rain_h_18',\n",
    "'target_normalized_19',\n",
    "'temperature_h_19',\n",
    "'rain_h_19',\n",
    "'target_normalized_20',\n",
    "'temperature_h_20',\n",
    "'rain_h_20',\n",
    "'target_normalized_21',\n",
    "'temperature_h_21',\n",
    "'rain_h_21',\n",
    "'target_normalized_22',\n",
    "'temperature_h_22',\n",
    "'rain_h_22',\n",
    "'target_normalized_23',\n",
    "'temperature_h_23',\n",
    "'rain_h_23',\n",
    "'es_festive_23',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'county',\n",
      "'is_business',\n",
      "'product_type',\n",
      "'target',\n",
      "'is_consumption',\n",
      "'datetime',\n",
      "'data_block_id_x',\n",
      "'row_id',\n",
      "'prediction_unit_id',\n",
      "'weekday',\n",
      "'hour_of_day',\n",
      "'day_of_month',\n",
      "'month',\n",
      "'date',\n",
      "'es_festive',\n",
      "'product_type_county_is_business',\n",
      "'eic_count',\n",
      "'installed_capacity',\n",
      "'data_block_id_y',\n",
      "'latitude',\n",
      "'longitude',\n",
      "'mergecolumn',\n",
      "'forecast_datetime_f',\n",
      "'temperature_f',\n",
      "'dewpoint_f',\n",
      "'cloudcover_high_f',\n",
      "'cloudcover_low_f',\n",
      "'cloudcover_mid_f',\n",
      "'cloudcover_total_f',\n",
      "'10_metre_u_wind_component_f',\n",
      "'10_metre_v_wind_component_f',\n",
      "'direct_solar_radiation_f',\n",
      "'surface_solar_radiation_downwards_f',\n",
      "'snowfall_f',\n",
      "'total_precipitation_f',\n",
      "'latitude_f',\n",
      "'longitude_f',\n",
      "'origin_datetime_f',\n",
      "'hours_ahead_f',\n",
      "'data_block_id_f',\n",
      "'forecast_datetime_f_original',\n",
      "'datetime_h',\n",
      "'temperature_h',\n",
      "'dewpoint_h',\n",
      "'rain_h',\n",
      "'snowfall_h',\n",
      "'surface_pressure_h',\n",
      "'cloudcover_total_h',\n",
      "'cloudcover_low_h',\n",
      "'cloudcover_mid_h',\n",
      "'cloudcover_high_h',\n",
      "'windspeed_10m_h',\n",
      "'winddirection_10m_h',\n",
      "'shortwave_radiation_h',\n",
      "'direct_solar_radiation_h',\n",
      "'diffuse_radiation_h',\n",
      "'latitude_h',\n",
      "'longitude_h',\n",
      "'datetime_h_original',\n",
      "'data_block_id_h',\n",
      "'forecast_date',\n",
      "'euros_per_mwh',\n",
      "'origin_date',\n",
      "'data_block_id',\n",
      "'target_normalized',\n",
      "'target_normalized_0',\n",
      "'temperature_h_0',\n",
      "'rain_h_0',\n",
      "'target_normalized_1',\n",
      "'temperature_h_1',\n",
      "'rain_h_1',\n",
      "'target_normalized_2',\n",
      "'temperature_h_2',\n",
      "'rain_h_2',\n",
      "'target_normalized_3',\n",
      "'temperature_h_3',\n",
      "'rain_h_3',\n",
      "'target_normalized_4',\n",
      "'temperature_h_4',\n",
      "'rain_h_4',\n",
      "'target_normalized_5',\n",
      "'temperature_h_5',\n",
      "'rain_h_5',\n",
      "'target_normalized_6',\n",
      "'temperature_h_6',\n",
      "'rain_h_6',\n",
      "'target_normalized_7',\n",
      "'temperature_h_7',\n",
      "'rain_h_7',\n",
      "'target_normalized_8',\n",
      "'temperature_h_8',\n",
      "'rain_h_8',\n",
      "'target_normalized_9',\n",
      "'temperature_h_9',\n",
      "'rain_h_9',\n",
      "'target_normalized_10',\n",
      "'temperature_h_10',\n",
      "'rain_h_10',\n",
      "'target_normalized_11',\n",
      "'temperature_h_11',\n",
      "'rain_h_11',\n",
      "'target_normalized_12',\n",
      "'temperature_h_12',\n",
      "'rain_h_12',\n",
      "'target_normalized_13',\n",
      "'temperature_h_13',\n",
      "'rain_h_13',\n",
      "'target_normalized_14',\n",
      "'temperature_h_14',\n",
      "'rain_h_14',\n",
      "'target_normalized_15',\n",
      "'temperature_h_15',\n",
      "'rain_h_15',\n",
      "'target_normalized_16',\n",
      "'temperature_h_16',\n",
      "'rain_h_16',\n",
      "'target_normalized_17',\n",
      "'temperature_h_17',\n",
      "'rain_h_17',\n",
      "'target_normalized_18',\n",
      "'temperature_h_18',\n",
      "'rain_h_18',\n",
      "'target_normalized_19',\n",
      "'temperature_h_19',\n",
      "'rain_h_19',\n",
      "'target_normalized_20',\n",
      "'temperature_h_20',\n",
      "'rain_h_20',\n",
      "'target_normalized_21',\n",
      "'temperature_h_21',\n",
      "'rain_h_21',\n",
      "'target_normalized_22',\n",
      "'temperature_h_22',\n",
      "'rain_h_22',\n",
      "'target_normalized_23',\n",
      "'temperature_h_23',\n",
      "'rain_h_23',\n",
      "'es_festive_23',\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "tipos=trainpb.groupby('county').mean().index\n",
    "for tipo in tipos:\n",
    "    tmp=trainpb[trainpb['county']==tipo]\n",
    "    plt.scatter(trainpb['installed_capacity'],trainpb['target'],label=tipo,alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.close('all')\n",
    "for item in dict:\n",
    "    df=dict[item]\n",
    "    df=df.sort_values('datetime')\n",
    "    plt.plot(df['datetime'],df['target'],label=item,markersize=0.8,marker='o', linestyle='dashed',\n",
    "     linewidth=0.3)\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "trainpb=trainpb.sort_values('datetime')\n",
    "puis=trainpb.groupby('prediction_unit_id').mean().index\n",
    "for pui in puis:\n",
    "    df=trainpb[trainpb['prediction_unit_id']==pui]\n",
    "    plt.plot(df['datetime'],df['target']/df['installed_capacity'],label=pui)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "tipos=client.groupby('county').mean().index\n",
    "for tipo in tipos:\n",
    "    tmp=client[client['county']==tipo]\n",
    "    plt.scatter(client['eic_count'],client['installed_capacity'],label=tipo,alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#no correlation btw all info in client.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "trainpb=trainpb.sort_values('datetime')\n",
    "puis=trainpb.groupby('prediction_unit_id').mean().index\n",
    "for pui in puis:\n",
    "    df=trainpb[trainpb['prediction_unit_id']==pui]\n",
    "    plt.plot(df['datetime'],df['target'],label=pui)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=train.groupby(['prediction_unit_id']).mean()\n",
    "print(temp.index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
