{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext cudf.pandas\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import holidays\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#For kaggle, GPT-4 suggests:\n",
    "from flask import Flask, jsonify\n",
    "import kaggle\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/datasets', methods=['GET'])\n",
    "def get_datasets():\n",
    "    # This is a basic example. You should add error handling and more specific functionality.\n",
    "    datasets = kaggle.api.datasets_list()  # Use Kaggle's API to get dataset info\n",
    "    return jsonify(datasets)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def reference_score(true_values, predicted_values):\n",
    "    score = mean_absolute_error(true_values, predicted_values)\n",
    "    return score\n",
    "\n",
    "\n",
    "def apply_dbscan_clustering(df, columns):\n",
    "    # Check if columns are in the DataFrame\n",
    "    if not all(col in df.columns for col in columns):\n",
    "        raise ValueError(\"One or more specified columns are not in the DataFrame\")\n",
    "\n",
    "    # Standardizing the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns])\n",
    "\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN()\n",
    "    clusters = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "    # Add cluster labels to the DataFrame\n",
    "    df['cluster_label'] = clusters\n",
    "\n",
    "    return df, dbscan, scaler\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# clustered_df, trained_dbscan, scaler = apply_dbscan_clustering(df, ['column1', 'column2'])\n",
    "\n",
    "def predict_dbscan_clusters(new_df, columns, trained_dbscan, scaler):\n",
    "    # Check if columns are in the DataFrame\n",
    "    if not all(col in new_df.columns for col in columns):\n",
    "        raise ValueError(\"One or more specified columns are not in the DataFrame\")\n",
    "\n",
    "    # Standardizing the new data using the previously fitted scaler\n",
    "    scaled_data = scaler.transform(new_df[columns])\n",
    "\n",
    "    # Predict clusters using the trained DBSCAN model\n",
    "    new_clusters = trained_dbscan.fit_predict(scaled_data)\n",
    "\n",
    "    # Add cluster labels to the new DataFrame\n",
    "    new_df['predicted_cluster_label'] = new_clusters\n",
    "\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and merging data\n",
    "Here the [Kaggle dataset description](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_frequency(df,dtcol):\n",
    "    # Convert the 'datetime' column to a DatetimeIndex if it's not already\n",
    "    df[dtcol] = pd.to_datetime(df[dtcol])\n",
    "\n",
    "    # Set the 'datetime' column as the index of the DataFrame\n",
    "    df = df.set_index(dtcol)\n",
    "\n",
    "    # Infer the frequency of the DatetimeIndex\n",
    "    frequency = pd.infer_freq(df.index[:100])  # Using a slice of the index to infer frequency\n",
    "\n",
    "    print(f\"The inferred frequency of the datetime column is: {frequency}\")\n",
    "\n",
    "\n",
    "def find_festives_estonia(df,datetime):\n",
    "    es_holidays=holidays.Estonia()\n",
    "    df['es_festive']=0\n",
    "    for holiday in es_holidays:\n",
    "        df['es_festive'].iloc[df[datetime]==holiday]=1\n",
    "    return(df)\n",
    "\n",
    "def lat_long_county(n,what='all',data_path=''):\n",
    "    ##import weather_station_to_county_mapping!!\n",
    "    #df=pd.read_csv(data_path+r'\\weather_station_to_county_mapping.csv')\n",
    "    #df=df.set_index('county')\n",
    "    \n",
    "    county_dict={0:\"HARJUMAA\",\\\n",
    "                 1:\"HIIUMAA\",\\\n",
    "                2:\"IDA-VIRUMAA\",\\\n",
    "                3:\"J\\u00c4RVAMAA\",\\\n",
    "                4:\"J\\u00d5GEVAMAA\",\\\n",
    "                5:\"L\\u00c4\\u00c4NE-VIRUMAA\",\\\n",
    "                6:\"L\\u00c4\\u00c4NEMAA\",\\\n",
    "                7:\"P\\u00c4RNUMAA\",\\\n",
    "                8:\"P\\u00d5LVAMAA\",\\\n",
    "                9:\"RAPLAMAA\",\\\n",
    "                10:\"SAAREMAA\",\\\n",
    "                11:\"TARTUMAA\",\\\n",
    "                12:\"UNKNOWN\",\\\n",
    "                13:\"VALGAMAA\",\\\n",
    "                14:\"VILJANDIMAA\",\\\n",
    "                15:\"V\\u00d5RUMAA\"}\n",
    "    df=pd.DataFrame(index=county_dict.keys(), columns=['lat','long'])\n",
    "\n",
    "    df.loc[0]=[59.416665,24.749997]\n",
    "    df.loc[1]=[58.923955,22.591947]\n",
    "    df.loc[2]=[59.2166658,27.2999988]\n",
    "    df.loc[3]=[58.8833298,25.5499978]\n",
    "    df.loc[4]=[58.74667,26.39389]\n",
    "    df.loc[5]=[59.32564740,26.63369150]\n",
    "    df.loc[6]=[58.916663,23.749997]\n",
    "    df.loc[7]=[58.38588,24.49711]\n",
    "    df.loc[8]=[58.06028,27.06944]\n",
    "    df.loc[9]=[59.00722,24.79278]\n",
    "    df.loc[10]=[58.416665,22.583331]\n",
    "    df.loc[11]=[58.378025, 26.728493]\n",
    "    df.loc[13]=[57.77781,26.0473]\n",
    "    df.loc[14]=[58.36389,25.59]\n",
    "    df.loc[15]=[57.83389, 27.01944]\n",
    "    df['lat'].loc[12]=df['lat'].mean()\n",
    "    df['long'].loc[12]=df['long'].mean()\n",
    "    '''\n",
    "    for item in county_dict:\n",
    "        df['county_name']=county_dict[item]\n",
    "    '''\n",
    "    if what=='all':\n",
    "        res=df.loc[n]\n",
    "    elif what=='lat':\n",
    "        res=df.loc[n]\n",
    "        res=res['lat']\n",
    "    elif what=='long':\n",
    "        res=df.loc[n]\n",
    "        res=res['long']\n",
    "    \n",
    "    return(res)\n",
    "\n",
    "\n",
    "def lat_long_all(data_path=''):\n",
    "    ##import weather_station_to_county_mapping!!\n",
    "    #df=pd.read_csv(data_path+r'\\weather_station_to_county_mapping.csv')\n",
    "    #df=df.set_index('county')\n",
    "    colat='latitude'\n",
    "    colong='longitude'\n",
    "    county_dict={0:\"HARJUMAA\",\\\n",
    "                 1:\"HIIUMAA\",\\\n",
    "                2:\"IDA-VIRUMAA\",\\\n",
    "                3:\"J\\u00c4RVAMAA\",\\\n",
    "                4:\"J\\u00d5GEVAMAA\",\\\n",
    "                5:\"L\\u00c4\\u00c4NE-VIRUMAA\",\\\n",
    "                6:\"L\\u00c4\\u00c4NEMAA\",\\\n",
    "                7:\"P\\u00c4RNUMAA\",\\\n",
    "                8:\"P\\u00d5LVAMAA\",\\\n",
    "                9:\"RAPLAMAA\",\\\n",
    "                10:\"SAAREMAA\",\\\n",
    "                11:\"TARTUMAA\",\\\n",
    "                12:\"UNKNOWN\",\\\n",
    "                13:\"VALGAMAA\",\\\n",
    "                14:\"VILJANDIMAA\",\\\n",
    "                15:\"V\\u00d5RUMAA\"}\n",
    "    df=pd.DataFrame(index=county_dict.keys(), columns=[colat,colong])\n",
    "\n",
    "    df.loc[0]=[59.416665,24.749997]\n",
    "    df.loc[1]=[58.923955,22.591947]\n",
    "    df.loc[2]=[59.2166658,27.2999988]\n",
    "    df.loc[3]=[58.8833298,25.5499978]\n",
    "    df.loc[4]=[58.74667,26.39389]\n",
    "    df.loc[5]=[59.32564740,26.63369150]\n",
    "    df.loc[6]=[58.916663,23.749997]\n",
    "    df.loc[7]=[58.38588,24.49711]\n",
    "    df.loc[8]=[58.06028,27.06944]\n",
    "    df.loc[9]=[59.00722,24.79278]\n",
    "    df.loc[10]=[58.416665,22.583331]\n",
    "    df.loc[11]=[58.378025, 26.728493]\n",
    "    df.loc[13]=[57.77781,26.0473]\n",
    "    df.loc[14]=[58.36389,25.59]\n",
    "    df.loc[15]=[57.83389, 27.01944]\n",
    "    df[colat].loc[12]=df[colat].mean()\n",
    "    df[colong].loc[12]=df[colong].mean()\n",
    "    '''\n",
    "    for item in county_dict:\n",
    "        df['county_name']=county_dict[item]\n",
    "    '''\n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def lat_county(n):\n",
    "    lat=lat_long_county(n,what='lat')\n",
    "    return(lat)\n",
    "\n",
    "def long_county(n):\n",
    "    long=lat_long_county(n,what='long')\n",
    "    return(long)\n",
    "\n",
    "    \n",
    "    \n",
    "def findClosest(arr, n, target):\n",
    "    left, right = 0, n - 1\n",
    "    while left < right:\n",
    "        if abs(arr[left] - target) <= abs(arr[right] - target):\n",
    "            right -= 1\n",
    "        else:\n",
    "            left += 1\n",
    "    return (arr[left],arr[left+1])\n",
    "def weighted_value(x,xm,xM,ym,yM):\n",
    "    if x==xM or x==xm:\n",
    "        y=(ym+yM)/2\n",
    "    else:\n",
    "        y=(x-xm)/(xM-xm)*(yM-ym)+ym\n",
    "    y=np.round(y, 2)\n",
    "    return(y)\n",
    "\n",
    "def find_weather_on_lat_long_old(weather_df,lat,long,parameters_to_extract,parameters_to_keep,datetime='datetime',datetime2='datetime2',col_lat='latitude',col_long='longitude'):\n",
    "    #find closest lat and long in forecast weather\n",
    "    [latm,latM]=findClosest(weather_df.groupby(col_lat).mean().index,len(weather_df.groupby(col_lat).mean().index),lat)\n",
    "    [lonm,lonM]=findClosest(weather_df.groupby(col_long).mean().index,len(weather_df.groupby(col_long).mean().index),long)\n",
    "    #tmp_df is a dataframe containing all predictions for the little swquare latlong\n",
    "    #Useful to avoid data out of bound. \n",
    "    temp_df=weather_df[weather_df[col_lat]>=latm]\n",
    "    temp_df=temp_df[temp_df[col_lat]<=latM]\n",
    "    temp_df=temp_df[temp_df[col_long]<=lonM]\n",
    "    temp_df=temp_df[temp_df[col_long]>=lonm]\n",
    "    #predictions are from different days. I need to account for datetime\n",
    "    dates=temp_df.groupby(datetime).mean().index\n",
    "    df=pd.DataFrame(index=dates,columns=parameters_to_extract+parameters_to_keep)\n",
    "\n",
    "\n",
    "\n",
    "    for col in parameters_to_extract:\n",
    "        df[col]=np.nan\n",
    "        print(col)\n",
    "        for data in dates:\n",
    "            squared_data=temp_df[temp_df[datetime]==data]\n",
    "\n",
    "            if len(squared_data)>4:\n",
    "                #48 hours predictions mean that at midnight day 0 you have preds up to midnight3. Keep the latest. \n",
    "                indexlist=pd.to_datetime(squared_data[datetime2].values).day==np.max(pd.to_datetime(squared_data[datetime2].values).day)\n",
    "\n",
    "                squared_data=squared_data.loc[indexlist]\n",
    "\n",
    "            #find weighted parameter based on lat and long. Could be checked as below. \n",
    "\n",
    "            squared=squared_data[squared_data[col_long]==lonM]\n",
    "            T1=weighted_value(lat,latm,latM,\n",
    "                              squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "\n",
    "            squared=squared_data[squared_data[col_long]==lonm]\n",
    "\n",
    "            T2=weighted_value(lat,latm,latM,squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "            #place in the right spot of temp the right value.\n",
    "            df[col].loc[data]=weighted_value(long,lonm,lonM,float(T2),float(T1))\n",
    "            for colkeep in parameters_to_keep:\n",
    "                if colkeep==datetime:\n",
    "                    df[colkeep]=data\n",
    "                else:\n",
    "                    df[colkeep]=squared_data[colkeep].iloc[0]\n",
    "            df[col_lat]=lat\n",
    "            df[col_long]=long\n",
    "    #The final dataset contains one row per datetime and one column per parameter. \n",
    "    #Can me merged duplicating rows in the train dataset.\n",
    "    return(df)\n",
    "    '''\n",
    "    temp3=temp2 in lat ==latM\n",
    "    T3=weighted(lon,lonm,lonM,T@onm,T@lonM)\n",
    "    temp3=temp2 in lon ==latm\n",
    "    T4=weighted(lon,lonm,lonM,T@onm,T@lonM)\n",
    "    Tref2=weighted(lat,latm,latM,T4,T3)\n",
    "\n",
    "    if Tref1=! Tref2:\n",
    "    Tref=mean(Tref1,Tref2)\n",
    "    else print('ok')\n",
    "\n",
    "    temp[col].iloc[data]=Tref\n",
    "    '''\n",
    "    \n",
    "def find_weather_on_lat_long(weather_df, lat, long, parameters_to_extract, parameters_to_keep,\n",
    "                             datetime='datetime', datetime2='datetime2', col_lat='latitude', col_long='longitude'):\n",
    "    # Precomputed groups (call these outside and pass them to the function if they don't change)\n",
    "    lat_group = weather_df.groupby(col_lat).mean().index\n",
    "    long_group = weather_df.groupby(col_long).mean().index\n",
    "\n",
    "    [latm, latM] = findClosest(lat_group, len(lat_group), lat)\n",
    "    [lonm, lonM] = findClosest(long_group, len(long_group), long)\n",
    "\n",
    "    # Filter the DataFrame in a single step\n",
    "    temp_df = weather_df[(weather_df[col_lat].between(latm, latM)) & (weather_df[col_long].between(lonm, lonM))]\n",
    "\n",
    "    # Create DataFrame\n",
    "    dates = temp_df.groupby(datetime).mean().index\n",
    "    df = pd.DataFrame(index=dates, columns=parameters_to_extract + parameters_to_keep)\n",
    "    \n",
    "    for col in parameters_to_extract:\n",
    "        print('Extracting {}'.format(col))\n",
    "        df[col] = np.nan\n",
    "        for data in dates:\n",
    "            squared_data = temp_df[temp_df[datetime] == data]\n",
    "\n",
    "            if len(squared_data) > 4:\n",
    "                max_day = pd.to_datetime(squared_data[datetime2].values).day.max()\n",
    "                squared_data = squared_data[pd.to_datetime(squared_data[datetime2].values).day == max_day]\n",
    "\n",
    "            for col_long_val in [lonm, lonM]:\n",
    "                squared = squared_data[squared_data[col_long] == col_long_val]\n",
    "                T = weighted_value(lat, latm, latM,\n",
    "                                   squared[squared[col_lat] == latm][col].values,\n",
    "                                   squared[squared[col_lat] == latM][col].values)\n",
    "                df.at[data, col] = T if col_long_val == lonm else weighted_value(long, lonm, lonM, float(T), float(df.at[data, col]))\n",
    "            \n",
    "            for colkeep in parameters_to_keep:\n",
    "                if colkeep == datetime:\n",
    "                    df[colkeep] = data\n",
    "                else:\n",
    "                    df[colkeep] = squared_data[colkeep].iloc[0]\n",
    "\n",
    "            df[col_lat] = lat\n",
    "            df[col_long] = long\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_weather_and_train(train,weather_df,lat_lon_data_train,lat_lon_data_w):\n",
    "    #lists of columns should contain in the right order:\n",
    "    # latitude,longitude,data.\n",
    "    #the function merges rows which have the same lat and long and same data \n",
    "    # - they are alla vailable in same time and same space. The dat ablock id is not taken into consderation yet.\n",
    "    train['mergecolumn']=np.nan\n",
    "    weather_df['mergecolumn']=np.nan\n",
    "    for index in train.index:\n",
    "        tempdt=pd.to_datetime(train[lat_lon_data_train[2]].loc[index])\n",
    "        val='{}{}{}{}'.format(tempdt.year,tempdt.month,tempdt.day,tempdt.hour)\n",
    "        train['mergecolumn'].loc[index]='{}_{}_{}'.\\\n",
    "            format(train[lat_lon_data_train[0]].loc[index],\n",
    "                   train[lat_lon_data_train[1]].loc[index],\n",
    "                   val)\n",
    "    try:\n",
    "        weather_df=weather_df.drop(columns=[lat_lon_data_w[2]])\n",
    "    except:\n",
    "        print('No duplicates')\n",
    "    weather_df=weather_df.reset_index(allow_duplicates=True)\n",
    "    for index in weather_df.index:\n",
    "        tempdt=pd.to_datetime(weather_df[lat_lon_data_w[2]].loc[index])\n",
    "        val='{}{}{}{}'.format(tempdt.year,tempdt.month,tempdt.day,tempdt.hour)\n",
    "        weather_df['mergecolumn'].loc[index]='{}_{}_{}'.\\\n",
    "            format(weather_df[lat_lon_data_w[0]].loc[index],\n",
    "                   weather_df[lat_lon_data_w[1]].loc[index],\n",
    "                   val)\n",
    "\n",
    "    merged_df = pd.merge(train, weather_df, on=['mergecolumn'], how='inner')\n",
    "\n",
    "    #merged_df=merged_df.drop_duplicates(subset=[data], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    #merged_df=merged_df.drop(['mergecolumn'], axis=1)\n",
    "\n",
    "    return(merged_df)\n",
    "\n",
    "\n",
    "def generate_dataframe(init_df,col,item):\n",
    "    df=init_df[init_df[col]==item]\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.csv\n",
    "\n",
    "- *county* - An ID code for the county.\n",
    "- *is_business* - Boolean for whether or not the prosumer is a business.\n",
    "- *product_type* - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n",
    "- *target* - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n",
    "- *is_consumption* - Boolean for whether or not this row's target is consumption or production.\n",
    "- *datetime* - The Estonian time in EET (UTC+2) / EEST (UTC+3).\n",
    "- *data_block_id* - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n",
    "- *row_id* - A unique identifier for the row.\n",
    "- *prediction_unit_id* - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n",
    "\n",
    "*prediction_unit_id* should not be used as input as it changes. \n",
    "It is interesting to understand if the physical behaviour is influenced by county, business and product type: uding a clustering and exluding those infos, then checking if clusters overlap with the items will give us some additiona infos.\n",
    "\n",
    "### client.csv\n",
    "\n",
    "- *product_type*\n",
    "- *county* - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n",
    "- *eic_count* - The aggregated number of consumption points (EICs - European Identifier Code).\n",
    "- *installed_capacity* - Installed photovoltaic solar panel capacity in kilowatts.\n",
    "- *is_business* - Boolean for whether or not the prosumer is a business.\n",
    "- *date*\n",
    "- *data_block_id*\n",
    "\n",
    "Installed capacity can be used to normalize production data. Production type, county and is business as well as data block id shoudl match the previous. \n",
    "To match client to train, we need to remap *prediction_unit_id* by finding county+productiontype+biz in the train dataset.\n",
    "\n",
    "### electricity_prices.csv\n",
    "\n",
    "- *origin_date*\n",
    "- *forecast_date*\n",
    "- *euros_per_mwh* - The price of electricity on the day ahead markets in euros per megawatt hour.\n",
    "- *data_block_id*\n",
    "\n",
    "How should prices will help me?\n",
    "\n",
    "### forecast_weather.csv \n",
    "Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n",
    "\n",
    "- *[latitude/longitude]* - The coordinates of the weather forecast.\n",
    "- *origin_datetime* - The timestamp of when the forecast was generated.\n",
    "- *hours_ahead* - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n",
    "- *temperature* - The air temperature at 2 meters above ground in degrees Celsius.\n",
    "- *dewpoint* - The dew point temperature at 2 meters above ground in degrees Celsius.\n",
    "- *cloudcover_[low/mid/high/total]* - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total.\n",
    "- *10_metre_[u/v]_wind_component* - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second.\n",
    "- *data_block_id*\n",
    "- *forecast_datetime* - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead.\n",
    "- *direct_solar_radiation* - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the preceding hour, in watt-hours per square meter.\n",
    "- *surface_solar_radiation_downwards* - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, in watt-hours per square meter.\n",
    "- *snowfall* - Snowfall over the previous hour in units of meters of water equivalent.\n",
    "- *total_precipitation* - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the preceding hour, in units of meters.\n",
    "\n",
    "### historical_weather.csv \n",
    "Historic weather data.\n",
    "\n",
    "- *datetime*\n",
    "- *temperature*\n",
    "- *dewpoint*\n",
    "- *rain* - Different from the forecast conventions. The rain from large scale weather systems of the preceding hour in millimeters.\n",
    "- *snowfall* - Different from the forecast conventions. Snowfall over the preceding hour in centimeters.\n",
    "- *surface_pressure* - The air pressure at surface in hectopascals.\n",
    "- *cloudcover_[low/mid/high/total]* - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n",
    "- *windspeed_10m* - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n",
    "- *winddirection_10m* - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n",
    "- *shortwave_radiation* - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n",
    "- *direct_solar_radiation*\n",
    "- *diffuse_radiation* - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n",
    "- *[latitude/longitude]* - The coordinates of the weather station.\n",
    "- *data_block_id*\n",
    "\n",
    "### Other data\n",
    "*public_timeseries_testing_util*.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it.\n",
    "\n",
    "*example_test_files/* Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three data_block_ids are repeats of the last three data_block_ids in the train set.\n",
    "\n",
    "*example_test_files/sample_submission.csv* A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission.\n",
    "\n",
    "*example_test_files/revealed_targets.csv* The actual target values from the day before the forecast time. This amounts to two days of lag relative to the prediction times in the test.csv.\n",
    "\n",
    "*enefit/* Files that enable the API. Expect the API to deliver all rows in under 15 minutes and to reserve less than 0.5 GB of memory. The copy of the API that you can download serves the data from example_test_files/. You must make predictions for those dates in order to advance the API but those predictions are not scored. Expect to see roughly three months of data delivered initially and up to ten months of data by the end of the forecasting period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######MAIN######\n",
    "data_path=r'C:\\Users\\Mipu_10\\Documents\\GitHub\\kaggle_enefit_prosumer_forecasting\\predict-energy-behavior-of-prosumers'\n",
    "\n",
    "'''\n",
    "##WEATHER IMPORTING AND MANIPULATION\n",
    "forecast_weather=pd.read_csv(data_path+r'\\forecast_weather.csv')\n",
    "historical_weather=pd.read_csv(data_path+r'\\historical_weather.csv')\n",
    "\n",
    "for col in forecast_weather.columns:\n",
    "    forecast_weather=forecast_weather.rename(columns={col: \"{}_f\".format(col)})\n",
    "    \n",
    "for col in historical_weather.columns:\n",
    "    historical_weather=historical_weather.rename(columns={col: \"{}_h\".format(col)})\n",
    "    \n",
    "electricity_prices['forecast_date']=pd.to_datetime(electricity_prices['forecast_date'])\n",
    "#create the final dataframe. \n",
    "all_h_w_cols=['datetime_h', 'temperature_h', 'dewpoint_h', 'rain_h', 'snowfall_h',\n",
    "       'surface_pressure_h', 'cloudcover_total_h', 'cloudcover_low_h',\n",
    "       'cloudcover_mid_h', 'cloudcover_high_h', 'windspeed_10m_h',\n",
    "       'winddirection_10m_h', 'shortwave_radiation_h',\n",
    "       'direct_solar_radiation_h', 'diffuse_radiation_h', 'latitude_h',\n",
    "       'longitude_h', 'data_block_id_h']\n",
    "\n",
    "all_f_w_cols=['latitude_f', 'longitude_f', 'origin_datetime_f', 'hours_ahead_f',\n",
    "       'temperature_f', 'dewpoint_f', 'cloudcover_high_f', 'cloudcover_low_f',\n",
    "       'cloudcover_mid_f', 'cloudcover_total_f', '10_metre_u_wind_component_f',\n",
    "       '10_metre_v_wind_component_f', 'data_block_id_f', 'forecast_datetime_f',\n",
    "       'direct_solar_radiation_f', 'surface_solar_radiation_downwards_f',\n",
    "       'snowfall_f', 'total_precipitation_f']\n",
    "\n",
    "\n",
    "final_dict={}\n",
    "colslat=['latitude','longitude']\n",
    "test_f=0\n",
    "test_h=0\n",
    "#item='cb'\n",
    "\n",
    "counties=train.groupby('county').mean().index\n",
    "lldf=lat_long_all() #contains one index per county, latitude and longitude columns. \n",
    "\n",
    "#max_datetime=pd.to_datetime('2021-11-01 00:00:00')\n",
    "#df=tot_df[tot_df['datetime']<max_datetime]\n",
    "'''\n",
    "\n",
    "'''\n",
    "for row in df.index:\n",
    "    try:\n",
    "        df[colslat[0]].iloc[row],df[colslat[1]].iloc[row]=lat_long_county(df['county'].iloc[row])\n",
    "    except:\n",
    "        df[colslat[0]].iloc[row],df[colslat[1]].iloc[row]=lat_long_county(df['county'].iloc[row].values)\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "print('I created lats and longs. Exploring weather.')\n",
    "\n",
    "#puis=df.groupby('prediction_unit_id').mean().index\n",
    "\n",
    "#df=df[df['prediction_unit_id']==puis[0]]\n",
    "\n",
    "lats=lldf.groupby('latitude').mean().index\n",
    "\n",
    "for lat in lats:\n",
    "    lons=lldf[lldf['latitude']==lat].groupby('longitude').mean().index\n",
    "    for long in lons:\n",
    "        print('{} and {}'.format(lat,long))\n",
    "        #find the dataset with all items at right lat and lon. \n",
    "\n",
    "        #forecasted\n",
    "        fweather_df=find_weather_on_lat_long(forecast_weather,lat,long,parameters_to_extract_f,parameters_to_keep_f,\n",
    "                                             datetime='forecast_datetime_f',datetime2='origin_datetime_f',col_lat='latitude_f',\n",
    "                                             col_long='longitude_f')\n",
    "        if test_f==0:\n",
    "            right_weather_forecast=fweather_df\n",
    "            test_f=1\n",
    "        else:\n",
    "            right_weather_forecast=pd.concat([right_weather_forecast,fweather_df])\n",
    "\n",
    "\n",
    "        #historical \n",
    "        hweather_df=find_weather_on_lat_long(historical_weather,lat,long,parameters_to_extract_h,parameters_to_keep_h,\n",
    "                                             datetime='datetime_h',datetime2='datetime_h',col_lat='latitude_h',\n",
    "                                             col_long='longitude_h')\n",
    "        if test_h==0:\n",
    "            right_weather_history=hweather_df\n",
    "            test_h=1\n",
    "        else:\n",
    "            right_weather_history=pd.concat([right_weather_history,hweather_df])\n",
    "            display(right_weather_history)\n",
    "\n",
    "\n",
    "        print('Found parameters for {} and {}'.format(lat,long))\n",
    "        #merge the right dweather df with train df\n",
    "\n",
    "        \n",
    "###--->> https://www.digitalocean.com/community/tutorials/pandas-dataframe-apply-examples impara qui a usarlo\n",
    "        \n",
    "        \n",
    "right_weather_forecast=right_weather_forecast.rename(columns={\"datetime_h\": \"datetime_h_original\"})\n",
    "right_weather_history=right_weather_history.rename(columns={\"forecast_datetime_f\": \"forecast_datetime_f_original\"})\n",
    "\n",
    "right_weather_forecast.to_pickle(\"./right_weather_forecast.pkl\")\n",
    "right_weather_history.to_pickle(\"./right_weather_history.pkl\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OTHER DATASETS\n",
    "\n",
    "train=pd.read_csv(data_path+r'\\train.csv')\n",
    "client=pd.read_csv(data_path+r'\\client.csv')\n",
    "electricity_prices=pd.read_csv(data_path+r'\\electricity_prices.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "right_weather_forecast=joblib.load(\"./right_weather_forecast.pkl\")\n",
    "right_weather_history=joblib.load(\"./right_weather_history.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order the train dataset based on prediction_unit_id\n",
    "train=train.sort_values('prediction_unit_id')\n",
    "\n",
    "#Split consumption and generation\n",
    "consumption=generate_dataframe(train,'is_consumption',1)\n",
    "production=generate_dataframe(train,'is_consumption',0)\n",
    "\n",
    "consumption_b=generate_dataframe(consumption,'is_business',1)\n",
    "consumption_c=generate_dataframe(consumption,'is_business',0)\n",
    "production_b=generate_dataframe(production,'is_business',1)\n",
    "production_c=generate_dataframe(production,'is_business',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train[train['prediction_unit_id']==3].sort_values('datetime')[['target','datetime','is_consumption']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#forecast\n",
    "parameters_to_extract_f=['temperature_f', 'dewpoint_f', 'cloudcover_high_f', 'cloudcover_low_f',\n",
    "   'cloudcover_mid_f', 'cloudcover_total_f', '10_metre_u_wind_component_f',\n",
    "   '10_metre_v_wind_component_f','direct_solar_radiation_f', 'surface_solar_radiation_downwards_f',\n",
    "   'snowfall_f', 'total_precipitation_f']\n",
    "\n",
    "parameters_to_keep_f=['latitude_f', 'longitude_f', 'origin_datetime_f', \n",
    "                    'hours_ahead_f','data_block_id_f', 'forecast_datetime_f']#these need to be kept not weighted!\n",
    "\n",
    "#historical\n",
    "parameters_to_extract_h=['temperature_h', 'dewpoint_h', 'rain_h', 'snowfall_h',\n",
    "   'surface_pressure_h', 'cloudcover_total_h', 'cloudcover_low_h',\n",
    "   'cloudcover_mid_h', 'cloudcover_high_h', 'windspeed_10m_h',\n",
    "   'winddirection_10m_h', 'shortwave_radiation_h',\n",
    "   'direct_solar_radiation_h', 'diffuse_radiation_h']\n",
    "\n",
    "parameters_to_keep_h=['latitude_h', 'longitude_h', 'datetime_h', \n",
    "                    'data_block_id_h']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#items of dictionary, will contain one dataframe per single production unit. Easier to manage. \n",
    "#dividing in 4 df is faster coz we donot work on very big dataframes. \n",
    "electricity_prices['forecast_date']=pd.to_datetime(electricity_prices['forecast_date'])\n",
    "\n",
    "dfs=[consumption_b,consumption_c,production_b,production_c]\n",
    "what=['cb','cc','pb','pc']\n",
    "dict={}\n",
    "#ii=0\n",
    "#for df in dfs:\n",
    "for is_consumption in [0,1]:\n",
    "    df=generate_dataframe(train,'is_consumption',is_consumption)\n",
    "    puis=df.groupby('prediction_unit_id').mean().index\n",
    "    \n",
    "    final_df=pd.DataFrame()\n",
    "    #iters=0\n",
    "    for pui in puis:\n",
    "        temp=df[df['prediction_unit_id']==pui]\n",
    "        temp=temp.sort_values('datetime')\n",
    "        temp['datetime']=pd.to_datetime(temp['datetime'])\n",
    "        temp['weekday']=pd.to_datetime(temp['datetime'].values).weekday\n",
    "        temp['hour_of_day']=pd.to_datetime(temp['datetime'].values).hour\n",
    "        temp['day_of_month']=pd.to_datetime(temp['datetime'].values).day\n",
    "        temp['month']=pd.to_datetime(temp['datetime'].values).month\n",
    "        temp['date']=pd.to_datetime(temp['datetime'].values).date\n",
    "        temp=find_festives_estonia(temp,'datetime')\n",
    "\n",
    "        dict['{}_{}'.format(pui,is_consumption)]=temp\n",
    "    \n",
    "#To avoid memory error, should truncate based on prediction unit or datetime. \n",
    "#Probably, the best thing to do is managing one prediction unit per time, and then batch training. \n",
    "lat_lon_data_train=['latitude','longitude','datetime']\n",
    "lat_lon_data_f=['latitude_f','longitude_f','forecast_datetime_f']\n",
    "lat_lon_data_h=['latitude_h','longitude_h','datetime_h']\n",
    "lat_lon_data_all=lat_lon_data_train+lat_lon_data_f\n",
    "\n",
    "\n",
    "target_shifts=24\n",
    "norm='target_normalized'\n",
    "cols_to_shift_hourly=[norm,'temperature_f',\n",
    "'cloudcover_high_f',\n",
    "'cloudcover_low_f',\n",
    "'cloudcover_mid_f',\n",
    "'cloudcover_total_f',\n",
    "'direct_solar_radiation_f',\n",
    "'snowfall_f',\n",
    "'total_precipitation_f']\n",
    "cols_to_shift_daily=['es_festive']\n",
    "\n",
    "\n",
    "#####CHANGE HERE FOR WHOLE DATASET\n",
    "#puis=list(dict.keys())[0:3]\n",
    "puis=dict.keys()\n",
    "\n",
    "save_id='all_puis'\n",
    "final_dict={}\n",
    "\n",
    "for pui in puis: \n",
    "    print('--- Doing prediction unit {}'.format(pui))\n",
    "    df=dict[pui]\n",
    "    client['date']=pd.to_datetime(client['date'].values).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['date']=pd.to_datetime(df['date'].values).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    client['date']=client['date'].astype(str)\n",
    "    df['date']=df['date'].astype(str)\n",
    "\n",
    "    print(np.shape(client))\n",
    "\n",
    "    tempmerged=pd.merge(df, client, left_on=['product_type','county','is_business','date'], right_on=['product_type','county','is_business','date'])#, how='join_type')\n",
    "    \n",
    "    print('After merging with client:')\n",
    "    display(tempmerged[tempmerged['prediction_unit_id']==pui].sort_values('datetime')[['target','datetime']])\n",
    "    dict[pui]=tempmerged\n",
    "    \n",
    "    df=dict[pui]\n",
    "    print('Defining lat and long')\n",
    "\n",
    "    df['latitude']=df['county']\n",
    "    df['latitude']= df['county'].apply(lat_county)\n",
    "\n",
    "\n",
    "    df['longitude']=df['county']\n",
    "    df['longitude'] = df['county'].apply(long_county)\n",
    "\n",
    "    right_weather_forecast[lat_lon_data_f[2]]=pd.to_datetime(right_weather_forecast.index)\n",
    "    right_weather_history[lat_lon_data_h[2]]=pd.to_datetime(right_weather_history.index)\n",
    "\n",
    "    tempp=merge_weather_and_train(df,right_weather_forecast,lat_lon_data_train,lat_lon_data_f)\n",
    "    #tempp=pd.merge(df, right_weather_forecast, left_on=lat_lon_data_train, right_on=lat_lon_data_f)#, how='join_type')\n",
    "\n",
    "    final_dict[pui]=merge_weather_and_train(tempp,right_weather_history,lat_lon_data_all,lat_lon_data_h)\n",
    "    #final_dict[pui]=pd.merge(tempp, right_weather_history, left_on=lat_lon_data_all, right_on=lat_lon_data_h)\n",
    "    final_dict[pui]=pd.merge(final_dict[pui], electricity_prices,  \n",
    "                               how='left', left_on=['datetime'], right_on = ['forecast_date'])\n",
    "    print('finished merging {}.'.format(pui))\n",
    "    #joblib.dump(final_dict, \"./final_dict.pkl\")\n",
    "    \n",
    "        \n",
    "    df=final_dict[pui]\n",
    "    df[norm]=df['target']/df['installed_capacity']\n",
    "    for shift in range(1,target_shifts+1):\n",
    "        for col in cols_to_shift_hourly:\n",
    "            df['{}_{}'.format(col,shift)]=df[col].shift(shift)\n",
    "            df['{}_{}'.format(col,shift)]=df[col].shift(shift)\n",
    "    for col in cols_to_shift_daily:\n",
    "        df['{}_{}'.format(col,shift)]=df[col].shift(target_shifts+1)\n",
    "    final_dict[pui]=df\n",
    "    #display(final_dict[pui])\n",
    "    print('finished!')\n",
    "    joblib.dump(final_dict, \"./train_dataset_complete_{}.pkl\".format(save_id))\n",
    "    print('saved!')\n",
    "print('FFFFFFINISSSSSH!!!!')\n",
    "\n",
    "for col in df.columns:\n",
    "    print(\"'{}',\".format(col))\n",
    "\n",
    "\n",
    "##TODO\n",
    "    # Capire dove nella trasformazione brucia il datetime doppio e lo fa diventare singolo\n",
    "    # Capire come gestire target che va su e giu per la stessa ora\n",
    "    # riallenare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['target'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
