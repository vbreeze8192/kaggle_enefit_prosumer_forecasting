{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cudf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b6bb443f34ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cudf.pandas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<c:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\decorator.py:decorator-gen-58>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\magics\\extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mipu_10\\appdata\\local\\programs\\python\\python38\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cudf'"
     ]
    }
   ],
   "source": [
    "%load_ext cudf.pandas\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import holidays\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#For kaggle, GPT-4 suggests:\n",
    "from flask import Flask, jsonify\n",
    "import kaggle\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/datasets', methods=['GET'])\n",
    "def get_datasets():\n",
    "    # This is a basic example. You should add error handling and more specific functionality.\n",
    "    datasets = kaggle.api.datasets_list()  # Use Kaggle's API to get dataset info\n",
    "    return jsonify(datasets)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def reference_score(true_values, predicted_values):\n",
    "    score = mean_absolute_error(true_values, predicted_values)\n",
    "    return score\n",
    "\n",
    "\n",
    "def apply_dbscan_clustering(df, columns):\n",
    "    # Check if columns are in the DataFrame\n",
    "    if not all(col in df.columns for col in columns):\n",
    "        raise ValueError(\"One or more specified columns are not in the DataFrame\")\n",
    "\n",
    "    # Standardizing the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns])\n",
    "\n",
    "    # Apply DBSCAN clustering\n",
    "    dbscan = DBSCAN()\n",
    "    clusters = dbscan.fit_predict(scaled_data)\n",
    "\n",
    "    # Add cluster labels to the DataFrame\n",
    "    df['cluster_label'] = clusters\n",
    "\n",
    "    return df, dbscan, scaler\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# clustered_df, trained_dbscan, scaler = apply_dbscan_clustering(df, ['column1', 'column2'])\n",
    "\n",
    "def predict_dbscan_clusters(new_df, columns, trained_dbscan, scaler):\n",
    "    # Check if columns are in the DataFrame\n",
    "    if not all(col in new_df.columns for col in columns):\n",
    "        raise ValueError(\"One or more specified columns are not in the DataFrame\")\n",
    "\n",
    "    # Standardizing the new data using the previously fitted scaler\n",
    "    scaled_data = scaler.transform(new_df[columns])\n",
    "\n",
    "    # Predict clusters using the trained DBSCAN model\n",
    "    new_clusters = trained_dbscan.fit_predict(scaled_data)\n",
    "\n",
    "    # Add cluster labels to the new DataFrame\n",
    "    new_df['predicted_cluster_label'] = new_clusters\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Example usage:\n",
    "# new_df = pd.read_csv('new_data.csv')\n",
    "# predicted_df = predict_dbscan_clusters(new_df, ['column1', 'column2'], trained_dbscan, scaler)\n",
    "\n",
    "\n",
    "def train_lstm(df, input_columns, output_column, N, M):\n",
    "    # Create shifted columns for each input\n",
    "    for col in input_columns:\n",
    "        for n in range(1, N + 1):\n",
    "            df[f'{col}_shifted_{n}'] = df[col].shift(n)\n",
    "\n",
    "    # Create shifted columns for the output\n",
    "    for m in range(1, M + 1):\n",
    "        df[f'{output_column}_shifted_{m}'] = df[output_column].shift(m)\n",
    "\n",
    "    # Drop rows with NaN values (due to shifting)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Separate the original output column\n",
    "    original_output = df[output_column]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df.drop(columns=[output_column]))\n",
    "\n",
    "    # Prepare the dataset for the LSTM\n",
    "    X = df_scaled\n",
    "    y = original_output.values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Reshape input to be 3D [samples, timesteps, features] as required by LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(1, X_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f'Model Score (MSE): {score}')\n",
    "    return model, score, scaler\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')  # Load your DataFrame here\n",
    "# model, score, scaler = train_lstm(df, ['input1', 'input2'], 'output', N=3, M=1)\n",
    "\n",
    "\n",
    "def test_lstm(model, scaler, df, input_columns, output_column, N, M):\n",
    "    # Apply the same time shifts to input columns\n",
    "    for col in input_columns:\n",
    "        for n in range(1, N + 1):\n",
    "            df[f'{col}_shifted_{n}'] = df[col].shift(n)\n",
    "\n",
    "    # Apply the same time shifts to output column\n",
    "    for m in range(1, M + 1):\n",
    "        df[f'{output_column}_shifted_{m}'] = df[output_column].shift(m)\n",
    "\n",
    "    # Drop rows with NaN values (due to shifting)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Standardize the data using the scaler from training\n",
    "    df_scaled = scaler.transform(df.drop(columns=[output_column]))\n",
    "\n",
    "    # Reshape input to be 3D as required by LSTM\n",
    "    X = df_scaled.reshape((df_scaled.shape[0], 1, df_scaled.shape[1]))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "# new_df = pd.read_csv('new_data.csv')  # Load new data\n",
    "# predictions = test_lstm(trained_model, trained_scaler, new_df, ['input1', 'input2'], 'output', N=3, M=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and merging data\n",
    "Here the [Kaggle dataset description](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_frequency(df,dtcol):\n",
    "    # Convert the 'datetime' column to a DatetimeIndex if it's not already\n",
    "    df[dtcol] = pd.to_datetime(df[dtcol])\n",
    "\n",
    "    # Set the 'datetime' column as the index of the DataFrame\n",
    "    df = df.set_index(dtcol)\n",
    "\n",
    "    # Infer the frequency of the DatetimeIndex\n",
    "    frequency = pd.infer_freq(df.index[:100])  # Using a slice of the index to infer frequency\n",
    "\n",
    "    print(f\"The inferred frequency of the datetime column is: {frequency}\")\n",
    "\n",
    "\n",
    "def find_festives_estonia(df,datetime):\n",
    "    es_holidays=holidays.Estonia()\n",
    "    df['es_festive']=0\n",
    "    for holiday in es_holidays:\n",
    "        df['es_festive'].iloc[df[datetime]==holiday]=1\n",
    "    return(df)\n",
    "\n",
    "def lat_long_county(n,data_path=''):\n",
    "    ##import weather_station_to_county_mapping!!\n",
    "    #df=pd.read_csv(data_path+r'\\weather_station_to_county_mapping.csv')\n",
    "    #df=df.set_index('county')\n",
    "    \n",
    "    county_dict={0:\"HARJUMAA\",\\\n",
    "                 1:\"HIIUMAA\",\\\n",
    "                2:\"IDA-VIRUMAA\",\\\n",
    "                3:\"J\\u00c4RVAMAA\",\\\n",
    "                4:\"J\\u00d5GEVAMAA\",\\\n",
    "                5:\"L\\u00c4\\u00c4NE-VIRUMAA\",\\\n",
    "                6:\"L\\u00c4\\u00c4NEMAA\",\\\n",
    "                7:\"P\\u00c4RNUMAA\",\\\n",
    "                8:\"P\\u00d5LVAMAA\",\\\n",
    "                9:\"RAPLAMAA\",\\\n",
    "                10:\"SAAREMAA\",\\\n",
    "                11:\"TARTUMAA\",\\\n",
    "                12:\"UNKNOWN\",\\\n",
    "                13:\"VALGAMAA\",\\\n",
    "                14:\"VILJANDIMAA\",\\\n",
    "                15:\"V\\u00d5RUMAA\"}\n",
    "    df=pd.DataFrame(index=county_dict.keys(), columns=['lat','long'])\n",
    "\n",
    "    df.loc[0]=[59.416665,24.749997]\n",
    "    df.loc[1]=[58.923955,22.591947]\n",
    "    df.loc[2]=[59.2166658,27.2999988]\n",
    "    df.loc[3]=[58.8833298,25.5499978]\n",
    "    df.loc[4]=[58.74667,26.39389]\n",
    "    df.loc[5]=[59.32564740,26.63369150]\n",
    "    df.loc[6]=[58.916663,23.749997]\n",
    "    df.loc[7]=[58.38588,24.49711]\n",
    "    df.loc[8]=[58.06028,27.06944]\n",
    "    df.loc[9]=[59.00722,24.79278]\n",
    "    df.loc[10]=[58.416665,22.583331]\n",
    "    df.loc[11]=[58.378025, 26.728493]\n",
    "    df.loc[13]=[57.77781,26.0473]\n",
    "    df.loc[14]=[58.36389,25.59]\n",
    "    df.loc[15]=[57.83389, 27.01944]\n",
    "    df['lat'].loc[12]=df['lat'].mean()\n",
    "    df['long'].loc[12]=df['long'].mean()\n",
    "    '''\n",
    "    for item in county_dict:\n",
    "        df['county_name']=county_dict[item]\n",
    "    '''\n",
    "\n",
    "    return(df.loc[n])\n",
    "    \n",
    "def findClosest(arr, n, target):\n",
    "    left, right = 0, n - 1\n",
    "    while left < right:\n",
    "        if abs(arr[left] - target) <= abs(arr[right] - target):\n",
    "            right -= 1\n",
    "        else:\n",
    "            left += 1\n",
    "    return (arr[left],arr[left+1])\n",
    "def weighted_value(x,xm,xM,ym,yM):\n",
    "    if x==xM or x==xm:\n",
    "        y=(ym+yM)/2\n",
    "    else:\n",
    "        y=(x-xm)/(xM-xm)*(yM-ym)+ym\n",
    "    return(y)\n",
    "\n",
    "def find_weather_on_lat_long(weather_df,lat,long,parameters_to_extract,datetime='datetime',col_lat='latitude',col_long='longitude'):\n",
    "    #find closest lat and long in forecast weather\n",
    "    [latm,latM]=findClosest(weather_df.groupby(col_lat).mean().index,len(weather_df.groupby(col_lat).mean().index),lat)\n",
    "    [lonm,lonM]=findClosest(weather_df.groupby(col_long).mean().index,len(weather_df.groupby(col_long).mean().index),long)\n",
    "    #tmp_df is a dataframe containing all predictions for the little swquare latlong\n",
    "    #Useful to avoid data out of bound. \n",
    "    temp_df=weather_df[weather_df[col_lat]>=latm]\n",
    "    temp_df=temp_df[temp_df[col_lat]<=latM]\n",
    "    temp_df=temp_df[temp_df[col_long]<=lonM]\n",
    "    temp_df=temp_df[temp_df[col_long]>=lonm]\n",
    "    #predictions are from different days. I need to account for datetime\n",
    "    dates=temp_df.groupby(datetime).mean().index\n",
    "    df=pd.DataFrame(index=dates,columns=parameters_to_extract+parameters_to_keep)\n",
    "    df[parameters_to_keep]=weather_df[parameters_to_keep]\n",
    "\n",
    "\n",
    "\n",
    "    for col in parameters_to_extract:\n",
    "        df[col]=np.nan\n",
    "        print(col)\n",
    "        for data in dates:\n",
    "            squared_data=temp_df[temp_df[datetime]==data]\n",
    "\n",
    "            if len(squared_data)>4:\n",
    "                #48 hours predictions mean that at midnight day 0 you have preds up to midnight3. Keep the latest. \n",
    "                indexlist=pd.to_datetime(squared_data[datetime2].values).day==np.max(pd.to_datetime(squared_data[datetime2].values).day)\n",
    "\n",
    "                squared_data=squared_data.loc[indexlist]\n",
    "\n",
    "            #find weighted parameter based on lat and long. Could be checked as below. \n",
    "\n",
    "            squared=squared_data[squared_data[col_long]==lonM]\n",
    "            print('Squared data at {}'.format(lonM))\n",
    "            display(squared)\n",
    "            T1=weighted_value(lat,latm,latM,\n",
    "                              squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "\n",
    "            squared=squared_data[squared_data[col_long]==lonm]\n",
    "\n",
    "            T2=weighted_value(lat,latm,latM,squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "            print(T2)\n",
    "            #place in the right spot of temp the right value.\n",
    "            df[col].loc[data]=weighted_value(long,lonm,lonM,float(T2),float(T1))\n",
    "\n",
    "    #The final dataset contains one row per datetime and one column per parameter. \n",
    "    #Can me merged duplicating rows in the train dataset.\n",
    "    return(df)\n",
    "    '''\n",
    "    temp3=temp2 in lat ==latM\n",
    "    T3=weighted(lon,lonm,lonM,T@onm,T@lonM)\n",
    "    temp3=temp2 in lon ==latm\n",
    "    T4=weighted(lon,lonm,lonM,T@onm,T@lonM)\n",
    "    Tref2=weighted(lat,latm,latM,T4,T3)\n",
    "\n",
    "    if Tref1=! Tref2:\n",
    "    Tref=mean(Tref1,Tref2)\n",
    "    else print('ok')\n",
    "\n",
    "    temp[col].iloc[data]=Tref\n",
    "    '''\n",
    "\n",
    "def merge_weather_and_train(train,weather_df,lat_lon_data_train,lat_lon_data_w):\n",
    "    #lists of columns should contain in the right order:\n",
    "    # latitude,longitude,data.\n",
    "    #the function merges rows which have the same lat and long and same data \n",
    "    # - they are alla vailable in same time and same space. The dat ablock id is not taken into consderation yet. \n",
    "    for index in train.index:\n",
    "        train['mergecolumn'].loc[index]='{}_{}_{}'.\\\n",
    "            format(train[lat_lon_data_train[0]],train[lat_lon_data_train[1]],train[lat_lon_data_train[2]])\n",
    "    for index in weather_df.index:\n",
    "        weather_df['mergecolumn'].loc[index]='{}_{}_{}'.\\\n",
    "            format(weather_df[lat_lon_data_w[0]],weather_df[lat_lon_data_w[1]],weather_df[lat_lon_data_w[2]])\n",
    "    merged_df = pd.merge(train, weather_df, on=['mergecolumn'], how='inner')\n",
    "    merged_df.drop_duplicates(subset=[data], keep='first', inplace=True, ignore_index=True)\n",
    "    merged_df.drop(['mergecolumn'], axis=1)\n",
    "    return(merged_df)\n",
    "\n",
    "\n",
    "def generate_dataframe(init_df,col,item):\n",
    "    df=init_df[init_df[col]==item]\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.csv\n",
    "\n",
    "- *county* - An ID code for the county.\n",
    "- *is_business* - Boolean for whether or not the prosumer is a business.\n",
    "- *product_type* - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n",
    "- *target* - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n",
    "- *is_consumption* - Boolean for whether or not this row's target is consumption or production.\n",
    "- *datetime* - The Estonian time in EET (UTC+2) / EEST (UTC+3).\n",
    "- *data_block_id* - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n",
    "- *row_id* - A unique identifier for the row.\n",
    "- *prediction_unit_id* - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set.\n",
    "\n",
    "*prediction_unit_id* should not be used as input as it changes. \n",
    "It is interesting to understand if the physical behaviour is influenced by county, business and product type: uding a clustering and exluding those infos, then checking if clusters overlap with the items will give us some additiona infos.\n",
    "\n",
    "### client.csv\n",
    "\n",
    "- *product_type*\n",
    "- *county* - An ID code for the county. See county_id_to_name_map.json for the mapping of ID codes to county names.\n",
    "- *eic_count* - The aggregated number of consumption points (EICs - European Identifier Code).\n",
    "- *installed_capacity* - Installed photovoltaic solar panel capacity in kilowatts.\n",
    "- *is_business* - Boolean for whether or not the prosumer is a business.\n",
    "- *date*\n",
    "- *data_block_id*\n",
    "\n",
    "Installed capacity can be used to normalize production data. Production type, county and is business as well as data block id shoudl match the previous. \n",
    "To match client to train, we need to remap *prediction_unit_id* by finding county+productiontype+biz in the train dataset.\n",
    "\n",
    "### electricity_prices.csv\n",
    "\n",
    "- *origin_date*\n",
    "- *forecast_date*\n",
    "- *euros_per_mwh* - The price of electricity on the day ahead markets in euros per megawatt hour.\n",
    "- *data_block_id*\n",
    "\n",
    "How should prices will help me?\n",
    "\n",
    "### forecast_weather.csv \n",
    "Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts.\n",
    "\n",
    "- *[latitude/longitude]* - The coordinates of the weather forecast.\n",
    "- *origin_datetime* - The timestamp of when the forecast was generated.\n",
    "- *hours_ahead* - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n",
    "- *temperature* - The air temperature at 2 meters above ground in degrees Celsius.\n",
    "- *dewpoint* - The dew point temperature at 2 meters above ground in degrees Celsius.\n",
    "- *cloudcover_[low/mid/high/total]* - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total.\n",
    "- *10_metre_[u/v]_wind_component* - The [eastward/northward] component of wind speed measured 10 meters above surface in meters per second.\n",
    "- *data_block_id*\n",
    "- *forecast_datetime* - The timestamp of the predicted weather. Generated from origin_datetime plus hours_ahead.\n",
    "- *direct_solar_radiation* - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the preceding hour, in watt-hours per square meter.\n",
    "- *surface_solar_radiation_downwards* - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, in watt-hours per square meter.\n",
    "- *snowfall* - Snowfall over the previous hour in units of meters of water equivalent.\n",
    "- *total_precipitation* - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the preceding hour, in units of meters.\n",
    "\n",
    "### historical_weather.csv \n",
    "Historic weather data.\n",
    "\n",
    "- *datetime*\n",
    "- *temperature*\n",
    "- *dewpoint*\n",
    "- *rain* - Different from the forecast conventions. The rain from large scale weather systems of the preceding hour in millimeters.\n",
    "- *snowfall* - Different from the forecast conventions. Snowfall over the preceding hour in centimeters.\n",
    "- *surface_pressure* - The air pressure at surface in hectopascals.\n",
    "- *cloudcover_[low/mid/high/total]* - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n",
    "- *windspeed_10m* - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n",
    "- *winddirection_10m* - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n",
    "- *shortwave_radiation* - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n",
    "- *direct_solar_radiation*\n",
    "- *diffuse_radiation* - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n",
    "- *[latitude/longitude]* - The coordinates of the weather station.\n",
    "- *data_block_id*\n",
    "\n",
    "### Other data\n",
    "*public_timeseries_testing_util*.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it.\n",
    "\n",
    "*example_test_files/* Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three data_block_ids are repeats of the last three data_block_ids in the train set.\n",
    "\n",
    "*example_test_files/sample_submission.csv* A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission.\n",
    "\n",
    "*example_test_files/revealed_targets.csv* The actual target values from the day before the forecast time. This amounts to two days of lag relative to the prediction times in the test.csv.\n",
    "\n",
    "*enefit/* Files that enable the API. Expect the API to deliver all rows in under 15 minutes and to reserve less than 0.5 GB of memory. The copy of the API that you can download serves the data from example_test_files/. You must make predictions for those dates in order to advance the API but those predictions are not scored. Expect to see roughly three months of data delivered initially and up to ten months of data by the end of the forecasting period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###MAIN###\n",
    "\n",
    "data_path=r'C:\\Users\\Mipu_10\\Documents\\GitHub\\kaggle_enefit_prosumer_forecasting\\predict-energy-behavior-of-prosumers'\n",
    "train=pd.read_csv(data_path+r'\\train.csv')\n",
    "client=pd.read_csv(data_path+r'\\client.csv')\n",
    "\n",
    "'''Forecast weather, train and client can be mapped based on county, each county is near a latitude and longitude. \n",
    "We need to map lat an long for each county, exctract nearest lat long wetehr forecast, \n",
    "and concat forecast to client and train. \n",
    "\n",
    "Produce 2 different dataser for consumption and production coz different behaviours, and probably from biz e non biz.\n",
    "Once weather forecast is related to county, county should not be included in inputs. \n",
    "Prod type is an input.\n",
    "4 models (cc,cb,pc,pb).\n",
    "Installed capacity to normalize production. \n",
    "\n",
    "Need to understand how to map data block id\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order the train dataset based on prediction_unit_id\n",
    "train=train.sort_values('prediction_unit_id')\n",
    "\n",
    "#Split consumption and generation\n",
    "consumption=generate_dataframe(train,'is_consumption',1)\n",
    "production=generate_dataframe(train,'is_consumption',0)\n",
    "\n",
    "consumption_b=generate_dataframe(consumption,'is_business',1)\n",
    "consumption_c=generate_dataframe(consumption,'is_business',0)\n",
    "production_b=generate_dataframe(production,'is_business',1)\n",
    "production_c=generate_dataframe(production,'is_business',0)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "--> merge client and train based on production type, is business and county. \n",
    "client has one row per data (daily). Train has one row per datetime (hourly).\n",
    "ttt column can connect the two dataframes. Also, we need to export the only date on train to merge on ttt and date. \n",
    "\n",
    "--> merge new dataframe with historical and forecasted weather. \n",
    "see function above. \n",
    "--> merge electricity prices for the day\n",
    "understand why??\n",
    "\n",
    "the final dataframe will have the same timestamp (datetime) but different data_block_id.\n",
    "--> data_block_id needs to be taken into account in training and prediction.\n",
    " \n",
    "nel dataframe ci sono una riga per ogni datetime, con tutti i dati per quel datetime. \n",
    "MA se il datablock è quello della giornata successiva, quel dato non è disponibile per la giornata in oggetto. \n",
    "Nel forecast io voglio predire le successive 24 ore di production o consumption.\n",
    "Alcuni dati sono sempre uguali, tipo isbiz,county etc. Altri invece non sono disponibili, tipo historical data. \n",
    "forecast è sempre disponibile il giorno prima. \n",
    "\n",
    "In fase di training è necessario prendere come input solo il blocco dati il cui data_id block \n",
    "è = a datablock id target + 1 (il giorno di predizione il valore target non è ancora disponibile).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#items of dictionary, will contain one dataframe per combination production/Consumptio, biz/nonbiz\n",
    "#dividing in 4 df is faster coz we donot work on very big dataframes. \n",
    "dfs=[consumption_b,consumption_c,production_b,production_c]\n",
    "what=['cb','cc','pb','pc']\n",
    "dict={}\n",
    "ii=0\n",
    "for df in dfs:\n",
    "    prods=df.groupby('prediction_unit_id').mean().index\n",
    "    \n",
    "    final_df=pd.DataFrame()\n",
    "    iters=0\n",
    "    for prod in prods:\n",
    "        temp=df[df['prediction_unit_id']==prod]\n",
    "        temp=temp.sort_values('datetime')\n",
    "        temp['datetime']=pd.to_datetime(temp['datetime'])\n",
    "        temp['weekday']=pd.to_datetime(temp['datetime'].values).weekday\n",
    "        temp['hour_of_day']=pd.to_datetime(temp['datetime'].values).hour\n",
    "        temp['day_of_month']=pd.to_datetime(temp['datetime'].values).day\n",
    "        temp['month']=pd.to_datetime(temp['datetime'].values).month\n",
    "        temp['date']=pd.to_datetime(temp['datetime'].values).date\n",
    "        temp=find_festives_estonia(temp,'datetime')\n",
    "        if iters==0:\n",
    "            last=temp\n",
    "            iters=1\n",
    "        else:\n",
    "            last=pd.concat([last,temp])\n",
    "    dict['{}'.format(what[ii])]=last\n",
    "    ii=ii+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.close('all')\n",
    "for item in dict:\n",
    "    df=dict[item]\n",
    "    df=df.sort_values('datetime')\n",
    "    plt.plot(df['datetime'],df['target'],label=item,markersize=0.8,marker='o', linestyle='dashed',\n",
    "     linewidth=0.3)\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "tipos=client.groupby('county').mean().index\n",
    "for tipo in tipos:\n",
    "    tmp=client[client['county']==tipo]\n",
    "    plt.scatter(client['eic_count'],client['installed_capacity'],label=tipo,alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#no correlation btw all info in client.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid memory error, should truncate based on prediction unit or datetime. \n",
    "for item in dict: \n",
    "    print('Doing item {}'.format(item))\n",
    "    df=dict[item]\n",
    "    ttt='{}_{}_{}'.format('product_type','county','is_business')\n",
    "    client[ttt]=client['product_type']*100+client['county']*10+client['is_business']\n",
    "    df[ttt]=df['product_type']*100+df['county']*10+df['is_business']\n",
    "    iters=0\n",
    "    puis=df.groupby(ttt).mean().index\n",
    "    cols_to_import=['eic_count','installed_capacity','data_block_id',ttt]\n",
    "    max_datetime=pd.to_datetime('20229-01 00:00:00')\n",
    "    truncated_df=df[df['datetime']<max_datetime]\n",
    "    for pui in puis:\n",
    "        print('--- Doing prediction unit {}'.format(pui))\n",
    "        dfpui=truncated_df[truncated_df[ttt]==pui]\n",
    "        clientpui=client[client[ttt]==pui][cols_to_import]\n",
    "        tempmerged=pd.merge(dfpui,clientpui,on=ttt,how='left')\n",
    "        tempmerged=tempmerged.drop_duplicates(subset=['datetime'])\n",
    "        if iters==0:\n",
    "            merged=tempmerged\n",
    "            iters=1\n",
    "        else:\n",
    "            merged=pd.concat([merged,tempmerged])\n",
    "    dict[item]=merged\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_weather=pd.read_csv(data_path+r'\\forecast_weather.csv')\n",
    "historical_weather=pd.read_csv(data_path+r'\\historical_weather.csv')\n",
    "electricity_prices=pd.read_csv(data_path+r'\\electricity_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dict2={}\n",
    "colslat=['latitude','longitude']\n",
    "test=0\n",
    "for item in dict:\n",
    "    tot_df=dict[item]\n",
    "    max_datetime=pd.to_datetime('2021-11-01 00:00:00')\n",
    "    df=tot_df[tot_df['datetime']<max_datetime]\n",
    "    df[colslat[0]]=np.nan\n",
    "    df[colslat[1]]=np.nan\n",
    "    df=df.reset_index()\n",
    "    for row in df.index:\n",
    "        try:\n",
    "            df[colslat[0]].iloc[0],df[colslat[1]].iloc[0]=lat_long_county(df['county'].iloc[0])\n",
    "        except:\n",
    "            df[colslat[0]].iloc[0],df[colslat[1]].iloc[0]=lat_long_county(df['county'].iloc[0].values)\n",
    "    print('I created lats and longs. Merging with weather.')\n",
    "    lats=df.groupby('latitude').mean().index\n",
    "    lons=df.groupby('longitude').mean().index\n",
    "    puis=df.groupby('prediction_unit_id').mean().index\n",
    "    parameters_to_extract=['temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low',\n",
    "       'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component',\n",
    "       '10_metre_v_wind_component', \n",
    "       'direct_solar_radiation', 'surface_solar_radiation_downwards',\n",
    "       'snowfall', 'total_precipitation']\n",
    "    parameters_to_keep=['latitude', 'longitude', 'origin_datetime', \n",
    "                        'hours_ahead','data_block_id', 'forecast_datetime']#these need to be kept not weighted!\n",
    "    lat_lon_data_train=['latitude','longitude','datetime']\n",
    "    lat_lon_data_w=['latitude','longitude','forecast_datetime']\n",
    "    for lat in lats:\n",
    "        for long in lons:\n",
    "            #find the dataset with all items at right lat and lon. \n",
    "            fweather_df=find_weather_on_lat_long(forecast_weather,lat,long,parameters_to_extract,parameters_to_keep,datetime='forecast_datetime',col_lat='latitude',col_long='longitude')\n",
    "            display(fweather_df)\n",
    "            if test==0:\n",
    "                right_weather_forecast=fweather_df\n",
    "                test=1\n",
    "            else:\n",
    "                right_weather_forecast=pd.concat([right_weather_forecast,fweather_df])\n",
    "            print('Found parameters for {} and {}'.format(lat,long))\n",
    "            #merge the right dweather df with train df\n",
    "    dict2[item]=merge_weather_and_train(df,right_weather_forecast,lat_lon_data_train,lat_lon_data_w)\n",
    "        \n",
    "    print('finished merging.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucntion test\n",
    "parameters_to_extract=['temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low',\n",
    "   'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component',\n",
    "   '10_metre_v_wind_component', \n",
    "   'direct_solar_radiation', 'surface_solar_radiation_downwards',\n",
    "   'snowfall', 'total_precipitation']\n",
    "parameters_to_keep=['latitude', 'longitude', 'origin_datetime', \n",
    "                    'hours_ahead','data_block_id', 'forecast_datetime']#these need to be kept not weighted!\n",
    "\n",
    "datetime='forecast_datetime'\n",
    "datetime2='origin_datetime'\n",
    "col_lat='latitude'\n",
    "col_long='longitude'\n",
    "weather_df=forecast_weather\n",
    "\n",
    "[latm,latM]=findClosest(weather_df.groupby(col_lat).mean().index,len(weather_df.groupby(col_lat).mean().index),lat)\n",
    "[lonm,lonM]=findClosest(weather_df.groupby(col_long).mean().index,len(weather_df.groupby(col_long).mean().index),long)\n",
    "#tmp_df is a dataframe containing all predictions for the little swquare latlong\n",
    "#Useful to avoid data out of bound. \n",
    "temp_df=weather_df[weather_df[col_lat]>=latm]\n",
    "temp_df=temp_df[temp_df[col_lat]<=latM]\n",
    "temp_df=temp_df[temp_df[col_long]<=lonM]\n",
    "temp_df=temp_df[temp_df[col_long]>=lonm]\n",
    "#predictions are from different days. I need to account for datetime\n",
    "dates=temp_df.groupby(datetime).mean().index\n",
    "print(dates)\n",
    "df=pd.DataFrame(index=dates,columns=parameters_to_extract+parameters_to_keep)\n",
    "df[parameters_to_keep]=weather_df[parameters_to_keep]\n",
    "print('df')\n",
    "display(df)\n",
    "\n",
    "\n",
    "for col in parameters_to_extract:\n",
    "    df[col]=np.nan\n",
    "    print(col)\n",
    "    for data in dates:\n",
    "        squared_data=temp_df[temp_df[datetime]==data]\n",
    "        print('original')\n",
    "        display(squared_data)\n",
    "        if len(squared_data)>4:\n",
    "            #48 hours predictions mean that at midnight day 0 you have preds up to midnight3. Keep the latest. \n",
    "            indexlist=pd.to_datetime(squared_data[datetime2].values).day==np.max(pd.to_datetime(squared_data[datetime2].values).day)\n",
    "            print(indexlist)\n",
    "            print('after')\n",
    "            squared_data=squared_data.loc[indexlist]\n",
    "            \n",
    "        print('Squared data')\n",
    "        display(squared_data)\n",
    "        #find weighted parameter based on lat and long. Could be checked as below. \n",
    "        \n",
    "        squared=squared_data[squared_data[col_long]==lonM]\n",
    "        print('Squared data at {}'.format(lonM))\n",
    "        display(squared)\n",
    "        T1=weighted_value(lat,latm,latM,\n",
    "                          squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "        print('Input: {} - {} - {} - {} - {} '.format(lat,latm,latM,squared[squared[col_lat]==latm][col].values,\n",
    "                        squared[squared[col_lat]==latM][col].values))\n",
    "        print(col, ': ',T1)\n",
    "        \n",
    "        squared=squared_data[squared_data[col_long]==lonm]\n",
    "        print('Squared data at {}'.format(lonm))\n",
    "        display(squared)\n",
    "        T2=weighted_value(lat,latm,latM,squared[squared[col_lat]==latm][col].values,squared[squared[col_lat]==latM][col].values)\n",
    "        print(T2)\n",
    "        #place in the right spot of temp the right value.\n",
    "        df[col].loc[data]=weighted_value(long,lonm,lonM,float(T2),float(T1))\n",
    "        print(df[col].loc[data])\n",
    "#The final dataset contains one row per datetime and one column per parameter. \n",
    "#Can me merged duplicating rows in the train dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(squared_data[datetime2].values).day==np.max(pd.to_datetime(squared_data[datetime2].values).day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_data=temp_df[temp_df[datetime]==data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(squared_data[datetime2].values).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime2='origin_datetime'\n",
    "if len(squared_data)>4:\n",
    "    squared_data=squared_data[squared_data[datetime2]<data]\n",
    "for lat in lats:\n",
    "    for long in lons: \n",
    "        [latm,latM]=findClosest(forecast_weather.groupby(col_lat).mean().index,len(weather_df.groupby(col_lat).mean().index),lat)\n",
    "        [lonm,lonM]=findClosest(forecast_weather.groupby(col_long).mean().index,len(weather_df.groupby(col_long).mean().index),long)\n",
    "        print(lat,long)\n",
    "        display(forecast_weather[(forecast_weather[col_lat]>=latm) & (forecast_weather[col_lat]<=latM) &\n",
    "                          (forecast_weather[col_long]>=lonm) & (forecast_weather[col_long]<=lonM)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((long,lonm,lonM,T2,T1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[latm,latM]=findClosest(weather_df.groupby(col_lat).mean().index,len(weather_df.groupby(col_lat).mean().index),lat)\n",
    "print(latm,latM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[colslat[0]].iloc[0],df[colslat[1]].iloc[0]=lat_long_county(df['county'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[colslat[1]].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "tipos=trainpb.groupby('county').mean().index\n",
    "for tipo in tipos:\n",
    "    tmp=trainpb[trainpb['county']==tipo]\n",
    "    plt.scatter(trainpb['installed_capacity'],trainpb['target'],label=tipo,alpha=0.2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "trainpb=trainpb.sort_values('datetime')\n",
    "puis=trainpb.groupby('prediction_unit_id').mean().index\n",
    "for pui in puis:\n",
    "    df=trainpb[trainpb['prediction_unit_id']==pui]\n",
    "    plt.plot(df['datetime'],df['target']/df['installed_capacity'],label=pui)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "trainpb=trainpb.sort_values('datetime')\n",
    "puis=trainpb.groupby('prediction_unit_id').mean().index\n",
    "for pui in puis:\n",
    "    df=trainpb[trainpb['prediction_unit_id']==pui]\n",
    "    plt.plot(df['datetime'],df['target'],label=pui)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=train.groupby(['prediction_unit_id']).mean()\n",
    "print(temp.index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
